def transform_dataframes(transformed_df, ss, glueContext, compute_uuid_udf, utl):
    """
    Function to transform multiple DataFrames based on the provided schemas and logic.

    Args:
        transformed_df (DataFrame): The input PySpark DataFrame.
        ss (object): An object containing schema definitions and column mappings.
        glueContext (GlueContext): The AWS Glue context.
        compute_uuid_udf (UDF): A UDF to compute UUIDs.
        utl (module): A utility module containing helper functions like `flatten_nested_json_column` and `melt_dataframe`.

    Returns:
        dict: A dictionary of transformed DataFrames.
    """
    # Flatten the JSON columns
    transformed_json_df = transformed_df.select(*ss.jsoncol)
 
    # Flatten nested JSON columns
    for column, schema in [
        ("card_revenue", ss.card_revenue_schema),
        ("card_transactions_stability", ss.card_transactions_stability),
        ("associated_people", ss.associated_people_schema),
        #("industries", ss.industries_schema),
        #("company_structure", ss.company_structure_schema),
        ("technologies", ss.technologies_schema),
    ]:
        transformed_json_df = utl.flatten_nested_json_column(transformed_json_df, column, schema)

    # Explode and process specific columns
    transformed_json_df = transformed_json_df.withColumn("associated_people__title", explode(col("associated_people__titles"))).drop("associated_people__titles")
    #transformed_json_df = transformed_json_df.withColumn("associated_tax_ids", from_json(col("associated_tax_ids"), ss.associated_tax_ids_schema))
    #transformed_json_df = transformed_json_df.withColumn("associated_tax_id", explode(col("associated_tax_ids"))).drop("associated_tax_ids")
    #transformed_json_df = transformed_json_df.withColumn("registered_agents", from_json(col("registered_agents"), ss.registered_agents_schema))
    #transformed_json_df = transformed_json_df.withColumn("registered_agents", explode(col("registered_agents")))
    transformed_json_df = utl.flatten_nested_json_column(
        transformed_json_df,
        "registered_agents",
        ss.registered_agents_schema,
        explode_array=False)

    transformed_json_df.show()
    
    
    return transformed_json_df
    


transformed_dataframes = transform_dataframes(transformed_df, ss, glueContext, compute_uuid_udf, utl)


def flatten_nested_json_column(df, column_name, schema, explode_array=True):
    """
    Flattens a nested JSON column in a PySpark DataFrame.
    
    Args:
        df (DataFrame): The input PySpark DataFrame.
        column_name (str): The name of the JSON column to flatten.
        schema (StructType): The schema of the JSON column.
        explode_array (bool): Whether to explode arrays or join with semicolon.
    
    Returns:
        DataFrame: The DataFrame with the flattened JSON column.
    """
    try:
        # Parse the JSON column into structured data
        df = df.withColumn(column_name, from_json(col(column_name), schema))
        
        # If explode_array is False, join array elements with semicolon
        if not explode_array:
            df = df.withColumn(column_name, concat_ws(";", col(column_name)))
            return df
        
        # Explode the array to create one row per element
        df = df.withColumn(column_name, explode(col(column_name)))
        # Iterate over the fields in the schema
        for field in schema.elementType.fields:  # Use `elementType` for ArrayType
            field_name = field.name
            field_type = field.dataType
            # If the field is a nested struct, recursively flatten it
            if isinstance(field_type, StructType):
                for nested_field in field_type.fields:
                    nested_field_name = nested_field.name
                    df = df.withColumn(
                        f"{column_name}__{field_name}__{nested_field_name}",
                        col(f"{column_name}.{field_name}.{nested_field_name}")
                    )
            else:
                # If the field is not nested, extract it directly
                df = df.withColumn(f"{column_name}__{field_name}", col(f"{column_name}.{field_name}"))
        # Drop the original JSON column
        df = df.drop(column_name)
    except Exception as error:
        print(f"Error flattening column {column_name}: {error}")
    return df
