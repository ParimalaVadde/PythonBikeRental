# Libraries for setting up Glue Job
import sys
import re
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import functions as SqlFuncs
from pyspark.sql.types import *
from pyspark.sql.functions import col, broadcast, regexp_replace, length, when
from pyspark.sql.functions import when, col, broadcast, regexp_replace, expr, concat_ws, coalesce, nanvl, lit
from pyspark.sql.functions import *

# Libraries for running the the ETL framework
import base64
import time
import boto3
from botocore.exceptions import NoCredentialsError
import psycopg2
import pandas as pd
from io import StringIO # python3; python2: BytesIO
import yaml

# Function to read the configuration file
def read_config(source_name:str):
    # Read .yaml configuration file
    # try:
    source_name = source_name.lower()
    print(f"{'*'*20} Attempting... {'*'*20}")
    with open(f'etl_config_{source_name}.yaml', 'r') as file:
        config_data = yaml.load(file, Loader=yaml.SafeLoader)
    print(f"{'*'*20} Config file for {source_name} loaded...{'*'*20}")
    print("Config Data:",config_data)
    # except:
        # print("Could not load config file!!!")
    return config_data

def read_map_file(file_name):
    # Specify the path to the mapping file with respect to the current directory
    map_file = f"{file_name}.csv"
    map_df = pd.read_csv(map_file)
    lowerify_cols = [col for col in map_df.columns if col not in ['Filter_Condition']]
    map_df[lowerify_cols] = map_df[lowerify_cols].apply(lambda x: x.astype(str).str.lower())
    map_df = map_df.apply(lambda x: x.astype(str).str.strip())
    return map_df

class rds_ops:

    # Constructor for fetching the PostgreSQL DB details from payload and setting up spark session
    def __init__(self, spark, config_data:dict, map_df:pd.DataFrame):
        
        # Setting mapping file and config file
        self.config_data = config_data
        self.map_df = map_df

        # Spark session initiation
        self.spark = spark
        
        # PostgreSQL connection attributes
        print(f"{'*'*20} Assigning AWS connection details to class attributes... {'*'*20}")
        payload = self.config_data['payload']
        self.app_admin = payload.get('APP_ADMIN')
        self.rds_host = payload.get('RDS_HOST')
        self.database_name = payload.get('DATABASE_NAME')
        self.database_port = payload.get('DATABASE_PORT')
        self.database_region = payload.get('DATABASE_REGION')
        self.ca_cert_path = payload.get('ca_cert_path')

    # Function to generate authorization token for connection to the PostgreSQL DB
    def get_rds_session_token(self)->object:
        print('Inside get_rds_session_token method')
        session = boto3.Session(profile_name=None)

        # try:
        print('Attempting to generate DB token...')
        client = session.client('rds', region_name=self.database_region)
        token = client.generate_db_auth_token(DBHostname=self.rds_host, Port=self.database_port,
                                                DBUsername=self.app_admin, Region=self.database_region)
        print('Token generated succesfully!')                            
        print(' - Get_rds_session_token: ' + str(token))
        return token
        
        # except Exception as err:
        #     print("Could not generate authorization token!!!")
        #     print(repr(err))
        #     return None
    
    # Function to connect to the PostgreSQL DB. This function returns a connection object which would be necessary to execute SQL queries on the DB
    def get_rds_connection(self)->object:
        print(f"Inside get_rds_connection function...")
        
        # try:
        gen_token = self.get_rds_session_token()
        
        # Uncomment the below two statements to use manual token to connect to the DB 
        # token_manual = ''
        # print('Manual Token:    ' + str(token_manual))
        
        # Connection statement to the PostgreSQL DB
        print(f'Attempting connection with the database...')
        self.connection = psycopg2.connect(host=self.rds_host,
                                user=self.app_admin,
                                password=gen_token,
                                # password=token_manual,
                                port=self.database_port,
                                dbname=self.database_name,
                                sslmode="verify-full",
                                sslrootcert=self.ca_cert_path)
        print(f"Connection to database established...")
        # conn.autocommit = True
        # conn.close()
        # return 'Success!'
        return self.connection
        
        # except Exception as err:
        #     print(f"Connection to database failed!!!")
        #     print(repr(err))
        #     return None

    # Function to fetch data from the PostgreSQL DB
    def query_postgresql(self, schema_name:str, table_name:str, col_list:list):
        
        print("Inside of the Read from Aurora Postgres method.")
        # Generate the QUERY TABLE query dynamically based on fields
        fetch_str = ", ".join(f"{col}" for col in col_list)
        print("fetch_str",fetch_str)
        
        query_table_query_1 = f"SELECT {fetch_str} FROM {schema_name}.{table_name}"
        query_table_query_2 = f"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}' AND table_schema = '{schema_name}'"
        query_table_query_3 = f"SELECT data_type FROM information_schema.columns WHERE table_name = '{table_name}' AND table_schema = '{schema_name}'"
        print('SQL STATEMENT TO QUERY DATA FROM TABLE: ', query_table_query_1)
        print('SQL STATEMENT TO FETCH THE NAMES OF COLUMNS IN THE TABLE: ', query_table_query_2)
        print('SQL STATEMENT TO FETCH THE TYPES OF COLUMNS IN THE TABLE: ', query_table_query_3)

        #Fetching the columns names from the table and appending it to a list
        col_names = []
        #Fetching the data types of columns from the table first and appending it to a list
        data_types = []
        
        self.cursor = self.connection.cursor()
            
        # try:
        self.cursor.execute(query_table_query_2)
        # Fetch names of all columns from the desired table
        records = self.cursor.fetchall()
        
        print("Size of columns list from information schema:",len(records))
        for i in range(0,len(records)):
            col_names.append(records[i][0])
        print("Column names: ",col_names)

    
        self.cursor.execute(query_table_query_3)
        # Fetch data types of all columns from the desired table
        records = self.cursor.fetchall()
        
        print("Size of data type list from information schema:",len(records))
        for i in range(0,len(records)):
            data_types.append(records[i][0])
        print("Data types: ",data_types)
    
        
        self.cursor.execute(query_table_query_1)
        # Fetch all records from the desired table
        records = self.cursor.fetchall()
        # print(records)
        
        # Converting all the attributes to StringType() to give as read input to Spark Dataframe
        struct_list = []
        for col1 in col_list:
            for col2, typo in zip(col_names, data_types):
                if col1 == col2:
                    struct_list.append(self.define_structure(col2, typo))
        p_schema = StructType(struct_list)
        print("p_schema",p_schema)
        spark_df = self.spark.createDataFrame(records, p_schema)

        # except Exception as err:
        #     # print(f"Could not query the table!!!")
        #     print(repr(err))

        # finally:
        print(f"Data query done from PostgreSQL table: {table_name}")
        return spark_df

    # Auxiliar functions
    def equivalent_type(self, f):
        if f == 'datetime64[ns]': return TimestampType()
        elif f == 'int64': return LongType()
        elif f == 'int32': return IntegerType()
        elif f == 'float64': return DoubleType()
        elif f == 'float32': return FloatType()
        else: return StringType()
    
    def define_structure(self, string, format_type):
        try: typo = self.equivalent_type(format_type)
        except: typo = StringType()
        return StructField(string, typo)
    
    # Given pandas dataframe, it will return a spark's dataframe.
    def pandas_to_spark(self, pandas_df):
        columns = list(pandas_df.columns)
        types = list(pandas_df.dtypes)
        struct_list = []
        for column, typo in zip(columns, types): 
          struct_list.append(self.define_structure(column, typo))
        p_schema = StructType(struct_list)
        return self.spark.createDataFrame(pandas_df, p_schema)
    
    def create_sparkDF(self, col_list:list,  col_typ:list):
        columns = col_list
        types = col_typ
        struct_list = []
        for column, typo in zip(columns, types):
          struct_list.append(self.define_structure(column, typo))
        p_schema = StructType(struct_list)
        return self.spark.createDataFrame([], p_schema)

    def export_to_s3(self, data:object, filename:str):
        #df = pd.DataFrame(data)
        #df.to_csv(filename, index=False)
        #print(f'Data exported to {filename}')
        bucket = 'app-id-111597-dep-id-114116-uu-id-9x0jt94siuto' # already created on S3
        csv_buffer = StringIO()
        data.to_csv(csv_buffer)
        s3_resource = boto3.resource('s3')
        s3_resource.Object(bucket, f'df_{filename}.csv').put(Body=csv_buffer.getvalue())

    def batch_table(self, batch_size:int, table)->list:
        print(f'Batchsizing the tables into a batch size of {batch_size}')
        batched_lst = []

        # Schema generation for pyspark
        schema_list = []
        for col_name in table.columns:
            schema_list.append(StructField(f"{col_name}", StringType(), True))
        schema = StructType(schema_list)

        if table.count() < batch_size:
            batched_lst.append(table)
        else:
            # batched_lst = [table[i:i+self.batch_size] for i in range(0,len(table),self.batch_size)]
            row_obj = table.collect()
            batched_lst = [self.spark.createDataFrame(row_obj[i:i+batch_size], schema) for i in range(0,table.count(),batch_size)]
                
        print(f'Batchsizing the table for {table} is completed!')
        # print(f'Batchsizing the DFs  for {eUniversalTable} is completed!')
        return batched_lst

    def etl_extraction(self, cdc_flag:bool=False):
        # Extracting only the columns that are required for moving data to staging DB
        extract_dict = {}
        source_tables = self.map_df['SOURCE_SYSTEM_TABLE'].unique().tolist()
        source_table_list = [x.lower() for x in source_tables if str(x) != 'nan']
        print(source_table_list)

        for source_table in self.config_data['source_table_names']:
            extract_dict[f'{source_table}'] = [x.lower() for x in self.map_df.loc[self.map_df['SOURCE_SYSTEM_TABLE']==source_table]['SOURCE_SYSTEM_COLUMN'].unique().tolist() if str(x) != 'nan']

        self.land_tables_v1 = self.fetch_land_tables(extract_dict)

        if cdc_flag:
            for source_table in self.config_data['source_table_names']:
                extract_dict[f'{source_table}'] = [x.lower() for x in self.map_df.loc[self.map_df['SOURCE_SYSTEM_TABLE']==source_table]['SOURCE_SYSTEM_COLUMN'].unique().tolist() if str(x) != 'nan']

            self.land_tables_v2 = self.fetch_land_tables(extract_dict, True)
        
            self.cdc_dict = self.change_data_capture()
            return self.cdc_dict
        
        return self.land_tables_v1


    def fetch_land_tables(self, extract_dict:dict, hist_flag:bool=False)->dict:
       
        print("Entering fetch_land_tables...")
        # Extracting the data from Landing DB and storing it in land_tables dictionary
        self.land_tables = dict()
        for itable, icol_list in extract_dict.items():
            if hist_flag:
                temp_land_df = self.query_postgresql(self.config_data["fetch_schema"], f"{itable}_hist", icol_list)
                print(f'Step 1 (Initial Count):- Table Name -> {itable}_hist \t no. of records -> {temp_land_df.count()} \t no. of fields -> {len(temp_land_df.columns)}')
            else:
                temp_land_df = self.query_postgresql(self.config_data["fetch_schema"], itable, icol_list)
                print(f'Step 1 (Initial Count):- Table Name -> {itable} \t no. of records -> {temp_land_df.count()} \t no. of fields -> {len(temp_land_df.columns)}')
            
            # df = pd.read_sql_query(select_query,con=self.connection)
            # Extracting IDs on which source level dedup should take place
            print(f"{'*'*20} Deduplication started {'*'*20}")
            dedup_ids = []
            for x in self.config_data['source_id_map'][itable].values():
                dedup_ids.append(x[0])
            # Dropping rows with identical values dedup ids
            print("Dedup keys:", dedup_ids)
            temp_land_df = temp_land_df.drop_duplicates(dedup_ids)
            print(f"{'*'*20} Deduplication completed {'*'*20}")
            self.land_tables[itable] = temp_land_df
            print(f'Step 2 (Deduplication):- Table Name -> {itable} \t no. of records -> {temp_land_df.count()} \t no. of fields -> {len(temp_land_df.columns)}')

        # Process for filtering        
        for source_table_name in self.map_df.loc[self.map_df['FUNCTION_TYPE']=='filter']['SOURCE_SYSTEM_TABLE']:
            filter_str = ""
            filter_str += " and ".join(self.map_df.loc[(self.map_df['FUNCTION_TYPE']=='filter') & (self.map_df['SOURCE_SYSTEM_TABLE']==source_table_name)]['Filter_Condition'])
            # filter_str += ")"
            print("filter_str:",filter_str)           
            # table_filtered = self.land_tables[f'{source_table_name}'].filter(filter_str)
            table_filtered = self.land_tables[f'{source_table_name}'].withColumn("business_validation_reason", when(expr(filter_str), "accept").otherwise("filter reject"))
            self.land_tables[f'{source_table_name}'] = table_filtered
            # print("table_filt:", table_filtered.show())
            # print(f'Table Name -> {source_table_name} \t no. of records -> {table_filtered.count()} \t no. of fields -> {len(table_filtered.columns)}')
            print(f'Step 3 (Filtering):- Table Name -> {source_table_name} \t no. of records -> {table_filtered.where(table_filtered["business_validation_reason"] == "accept").count()} \t no. of fields -> {len(table_filtered.columns)}')

        # Standardization for applying data quality rules
        print("Starting with Data Quality Checks...")
        self.data_quality_check()
        print("Data Quality Check Completed!")

        # Performing records integrity on all the tables with respect to the accounts table
        print(f"{'*'*20} Data validation started {'*'*20}")
        for table_name, table in self.land_tables.items():
            table.createOrReplaceTempView(f"{table_name}")

        for table_name, table in self.land_tables.items():
            if table_name != f"{list(self.config_data['collate_key'].keys())[0]}":                
                collate_key = self.config_data['collate_key'][f"{table_name}"]
                print("collate_key:",collate_key)
                str_join = f"select a.{list(self.config_data['collate_key'].values())[0]} as main_id, a.business_validation_reason as val_rsn, b.* from {list(self.config_data['collate_key'].keys())[0]} as a full join {table_name} as b on a.{list(self.config_data['collate_key'].values())[0]}=b.{collate_key}"
                print("str_join:",str_join)
                table = self.spark.sql(f"{str_join}")
                table = table.withColumn("business_validation_reason", when((table["main_id"].isNull()) | (table["val_rsn"] != "accept"), "validation reject").otherwise(col("business_validation_reason")))
                table = table.drop(col("main_id"))
                table = table.drop(col("val_rsn"))
                # table = table.filter(table[f"{collate_key}"].isNotNull())
                self.land_tables[table_name] = table
                print(f'Step 5 (Business Integrity):- Table Name -> {table_name} \t no. of records -> {table.count()} \t no. of fields -> {len(table.columns)}')
        print(f"{'*'*20} Data validation completed {'*'*20}")

        print(f"Loading stats: Tables loaded from Landing : {len(self.land_tables), self.land_tables.keys()}")
        print("Record count from each Landing Dataframe: ")
        for tableName, datadf in self.land_tables.items():
            print(f'Table Name -> {tableName} \t no. of records -> {datadf.count()} \t no. of fields -> {len(datadf.columns)}')
        return self.land_tables

    def fetch_stage_table(self, stg_table_name: str):
       
        print("Entering fetch_stage_tables...")
        # Fetching the primary key id for the table from mapping file
        extract_list = [x.lower() for x in self.map_df.loc[(self.map_df['TARGET_TABLE']==stg_table_name) & (self.map_df['IS_KEY']=='y')]['TARGET_COLUMN'].unique().tolist() if str(x) != 'nan']
        print("Extract List:", extract_list)

        # Extracting the data from Staging DB
        existing_stg_df = self.query_postgresql(self.config_data["export_schema"], stg_table_name, extract_list)
        print("existing_stg_df", existing_stg_df.show())

        print(f'Table Name -> {stg_table_name} \t no. of records -> {existing_stg_df.count()} \t no. of fields -> {len(existing_stg_df.columns)}')

        return existing_stg_df
    
    def change_data_capture(self):

        # Initialising the CDC dictionary
        cdc_dict = {}

        # Looping through all the tables for change data capture
        for (table_name1, itable1), (table_name2, itable2) in zip(self.land_tables_v1.items(), self.land_tables_v2.items()):

            # Identify inserts and updates
            upserts = itable1.subtract(itable2).withColumn("business_validation_reason", when(col("business_validation_reason") != "accept", col("business_validation_reason")).otherwise("accept"))
            print("upserts:", upserts.show())
            
            # Identify deletes - this method is wrong as it captures not only deletes but also the updates that has happened in the current table
            # deletes = itable2.subtract(itable1).subtract(upserts).withColumn("business_validation_reason", when(col("business_validation_reason") != "accept", col("business_validation_reason")).otherwise("deleted"))
            # print("deletes:", deletes.show())

            # Identify deletes
            # Define the primary key column 
            primary_key_column = "id" 
            # Step 1: Select only the primary key 
            current_keys = itable1.select(primary_key_column) 
            hist_keys = itable2.select(primary_key_column) 
            # Step 2: Perform subtract to find deleted primary keys 
            deleted_keys = hist_keys.subtract(current_keys) 
            # Step 3: Filter historical_table1 to get only records that match the deleted primary keys 
            deletes = itable2.filter(itable2[primary_key_column].isin(deleted_keys.rdd.flatMap(lambda x: x).collect())).withColumn("business_validation_reason", when(col("business_validation_reason") != "accept", col("business_validation_reason")).otherwise("deleted"))
            print("deletes:", deletes.show())

            # Union all changes
            cdc_df = upserts.union(deletes)
            # Show changes 
            print("cdc_show:", cdc_df.show())
            cdc_dict[f"{table_name2}"] = cdc_df
            
        return cdc_dict
    
    def data_quality_check(self):
        # Creating a dataframe for dq attributes
        self.dq_attributes_df = self.map_df[self.map_df['DQ_CHECK'] == "y"]

        # Standardising the DQ attributes and then applying DQ rules
        if self.dq_attributes_df.shape[0] > 0:
            for idx, row in self.dq_attributes_df.iterrows():

                df_temp = self.land_tables[row["SOURCE_SYSTEM_TABLE"]]
                src_dq_col = row["SOURCE_SYSTEM_COLUMN"]
                df_temp = df_temp.withColumn(src_dq_col, regexp_replace(col(src_dq_col), r"[\D]", ""))
                print(f"{'*'*20} DQ check starting for {src_dq_col} {'*'*20}")
                if row["DQ_FUNC"] == "length":
                    if row["DQ_ACTION"] == "reject_value":
                        df_temp = df_temp.withColumn(src_dq_col, when(length(col(src_dq_col)) == row["DQ_RULE"], col(src_dq_col)).otherwise(""))
                    elif row["DQ_ACTION"] == "reject_record":
                        # df_temp = df_temp.filter(length(col(src_dq_col)) == row["DQ_RULE"])
                        df_temp = df_temp.withColumn("business_validation_reason", when(length(col(src_dq_col)) != row["DQ_RULE"], "DQ reject").otherwise(col("business_validation_reason")))

                self.land_tables[row["SOURCE_SYSTEM_TABLE"]] = df_temp
                print("temp df: ", df_temp.show())
                print(f'Step 4 (Data Quality Check):- Table Name -> {row["SOURCE_SYSTEM_TABLE"]} \t no. of records -> {df_temp.where(df_temp["business_validation_reason"] == "accept").count()} \t no. of fields -> {len(df_temp.columns)}')
        else:
            print(f"{'*'*20} No Attributes present in mapping file to perform DQ {'*'*20}")

        return None


    def generate_insert_statements(self, cursor:object, schema_name:str, table_name:str, df_data:object):
        ## df_data is the transformed table
        print('Inside the generate_insert_statements function...')
        
        if df_data.rdd.isEmpty():
            raise ValueError("DataFrame cannot be empty!!!")
        
        # Generate the QUERY TABLE query dynamically based on fields
        column_query = f"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}' AND table_schema = '{schema_name}'"
        print('SQL STATEMENT TO QUERY INFORMATION ABOUT PDM TABLE: ', column_query)

        #Fetching the columns names from the staging table and appending it to a list
        pdm_col_names = []

        cursor.execute(column_query)
        # Fetch names of all columns from the desired table
        records = cursor.fetchall()
        
        print("Size of PDM columns list from information schema: ",len(records))
        for i in range(0,len(records)):
            pdm_col_names.append(records[i][0])
        print("PDM Column names: ", pdm_col_names)
        
        # records = df_data.to_dict(orient='records')
        rows = df_data.collect()
        records = [row.asDict() for row in rows]
        print("records", records[0:5])
        # print("records",records)
        # keys will be the column names
        all_columns_in_transforms = records[0].keys() ## Additional column 'id'
        
        # Taking an intersection of the columns from transformed tables and columns present in the staging tables
        columns = [eColumn for eColumn in pdm_col_names if eColumn in all_columns_in_transforms]
        print("Common columns between transformed tables and staging tables: ",columns)

        insert_query = f"INSERT INTO {schema_name}.{table_name} ({', '.join(columns)}) VALUES "
        print('Initial Insert Query :- ', insert_query)
        
        # List to populate the value of all atrributes
        values_list = []
        for record in records:
            values = ','.join(f"'{record[col]}'" for col in columns)
            # print("Values : ", values)
            values_list.append(f"({values})")
        
        # print("Value List : ", values_list)
        insert_query += ', '.join(values_list).replace('\'None\'','NULL')
        print('INSERT QUERY Generated...')
        # print('Insert Query :- ', insert_query)
        
        return insert_query, len(values_list)

    def generate_update_statements(self, conn:object, cursor:object, schema_name:str, table_name:str, df_data:object, key:str):
        ## df_data is the transformed table
        print('Inside the generate_update_statements function...')
        
        if df_data.rdd.isEmpty():
            raise ValueError("DataFrame cannot be empty!!!")          
        
        # Generate the QUERY TABLE query dynamically based on fields
        column_query = f"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}' AND table_schema = '{schema_name}'"
        print('SQL STATEMENT TO QUERY INFORMATION ABOUT PDM TABLE: ', column_query)

        #Fetching the columns names from the staging table and appending it to a list
        pdm_col_names = []

        cursor.execute(column_query)
        # Fetch names of all columns from the desired table
        records = cursor.fetchall()
        
        print("Size of PDM columns list from information schema: ",len(records))
        for i in range(0,len(records)):
            pdm_col_names.append(records[i][0])
        print("PDM Column names: ", pdm_col_names)
        
        # records = df_data.to_dict(orient='records')
        rows = df_data.collect()
        records = [row.asDict() for row in rows]
        # print("records",records)
        # keys will be the column names
        all_columns_in_transforms = records[0].keys() ## Additional column 'id'
        
        # Taking an intersection of the columns from transformed tables and columns present in the staging tables
        columns = [eColumn for eColumn in pdm_col_names if eColumn in all_columns_in_transforms]
        print("Common columns between transformed tables and staging tables: ",columns)

        # Changing the is active status to N for the records that have been deleted
        # if del_rec_status:
        #     update_query = f"UPDATE {schema_name}.{table_name} SET is_active = 'N' WHERE {key} in ("
        #     update_query += ', '.join(f"'{record[key]}'" for record in records)
        #     update_query += ')'
        #     # print('UPDATE QUERY Generated...')
        #     # print('Update Query :- ', update_query)
        #     cursor.execute(update_query)
        #     conn.commit()
        #     return update_query, len(records)
        
        # List to populate the value of all atrributes
        values_list = []
        for record in records:
            update_query = f"UPDATE {schema_name}.{table_name} SET "
            values = ', '.join(f"{col} = '{record[col]}'" for col in columns if col not in ['created_date', 'created_by'])
            update_query += ''.join(values).replace('\'None\'','NULL')
            # print("Values : ", values)
            values_list.append(f"({values})")
            update_query += ''.join(f" WHERE {key} = '{record[key]}'")
            # print('UPDATE QUERY Generated...')
            # print('Update Query :- ', update_query)
            cursor.execute(update_query)
            conn.commit()

        return update_query, len(values_list)
    


    def export_to_postgresql(self, conn:object, cursor:object, schema_name:str, table_name:str, df_data:object, key:str):

        print("Inside of the export_to_postgresql method...")

        # cursor.execute(f"TRUNCATE TABLE {schema_name}.{table_name}")
        # conn.commit()

        # Below code will extract the existing data from staging to segregate the ids that needs to be updated vs that needs to be inserted
        print(f"{'*'*20} Extract from Staging... {'*'*20}")
        spark_df_ids = self.fetch_stage_table(table_name)

        common_ids = df_data.select(key).intersect(spark_df_ids.select(key))
        print("common_ids:", common_ids.show())
        print("type of common_ids:", type(common_ids))

        list_values_ids = common_ids.select(key).rdd.flatMap(lambda x: x).collect()
        # print("list_values_ids", list_values_ids)
        print("len of list_values_ids", len(list_values_ids))
        
        # Initialize the counts
        number_of_records_inserted = 0
        number_of_records_updated = 0

        # UPDATE section
        if len(list_values_ids)>0:
            update_df = df_data.filter(col(f"{key}").isin(list_values_ids))
            print("update_df:", update_df.show())

            print(f"{'*'*20} Batch-sizing the data started {'*'*20}")
            batched_list = self.batch_table(50000, update_df)
            print('Number of batches of records after batch sizing for batchsize {} for table {} is {}'.format(50000, table_name, len(batched_list)))
            print(f"{'*'*50}")
            print('\n'*2)

            for count,batched_table in enumerate(batched_list, start=1):
                # Update data into PostgreSQL
                # Define the PostgreSQL UPDATE statement dynamically
                print(f"{'*'*20} Insert starting for batch {count}/{len(batched_list)} {'*'*20}")
                print(f"{'*'*50}")
                print('Generating the update query:')
                update_query, number_of_records_updated = self.generate_update_statements(conn, cursor, schema_name, table_name, batched_table, key)
                # print('Update Query: ' + update_query)
                print('number_of_records_updated: ', number_of_records_updated)
                print(f"{'*'*50}")
        

        # INSERT section
        insert_df = df_data.filter(~col(f"{key}").isin(list_values_ids))
        print("insert_df", insert_df.show())

        print(f"{'*'*20} Batch-sizing the data started {'*'*20}")
        batched_list = self.batch_table(50000, insert_df)
        print('Number of batches of records after batch sizing for batchsize {} for table {} is {}'.format(50000, table_name, len(batched_list)))
        print(f"{'*'*50}")
        print('\n'*2)

        for count,batched_table in enumerate(batched_list, start=1):
            # Insert data into PostgreSQL
            # Define the PostgreSQL INSERT statement dynamically
            print(f"{'*'*20} Insert starting for batch {count}/{len(batched_list)} {'*'*20}")
            if not batched_table.rdd.isEmpty():
                print(f"{'*'*50}")
                print('Generating the insert query:')
                insert_query, number_of_records_inserted = self.generate_insert_statements(cursor, schema_name, table_name, batched_table)
                # print('Insert Query: ' + insert_query)
                print('number_of_records_inserted: ', number_of_records_inserted)
                print(f"{'*'*50}")
                
                # try:
                cursor.execute(insert_query)
                conn.commit()

                # except Exception as err:
                #     print(f"Update/Insert failed!!!")
                #     print(repr(err))

        print(f"Data exported to PostgreSQL table: {table_name}")
        print(f'Data Inserted & Updated to xbsddbtest.{schema_name} for Table : {table_name}')
        print(f" Records Inserted ----> {number_of_records_inserted}")
        print(f" Records Updated ----> {number_of_records_updated}")
        print(f"{'*'*50}")
        print('\n'*1)
    
