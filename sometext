from pyspark.sql.functions import udf, struct, when, col
from pyspark.sql.types import StringType
import hashlib
import uuid

def compute_uuid(data_row, decimal_columns):
    values = []
    
    # Convert Row object to dictionary
    data = data_row.asDict()
    
    # Sort by column names to ensure consistent order
    for column in sorted(data.keys()):
        val = data[column]
        if val is not None and str(val).strip().lower() not in {"", "nan", "null", "none", "nat"}:
            if column in decimal_columns:
                val = f"{float(val):.6f}"
            values.append(str(val))
    
    name = ','.join(values)
    return str(uuid.UUID(bytes=hashlib.md5(name.encode()).digest())) if name else None

# Register UDF
compute_uuid_udf = udf(lambda x: compute_uuid(x, decimal_columns), StringType())

# Usage - works for both single and multiple columns
transformed_business_entity_details = transformed_business_entity_details.withColumn(
    "business_entity_details_id",
    compute_uuid_udf(struct(*transformed_business_entity_details.columns))
)

source_df = source_df.withColumn(
    "stg_business_entity_id",
    when(
        col("sds_supplier_id").isNotNull(), col("sds_supplier_id")
    ).otherwise(
        compute_uuid_udf(struct("vendor_name_cleaned"))
    )
)
