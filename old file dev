# Libraries for setting up Glue Job
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import functions as SqlFuncs
from pyspark.sql.types import *
from pyspark.sql.functions import col, broadcast

# Libraries for running the the ETL framework
import base64
import time
import boto3
from botocore.exceptions import NoCredentialsError
import psycopg2
import pandas as pd
from io import StringIO # python3; python2: BytesIO
import yaml
import etl_utils as utils
from etl_utils import rds_ops
from etl_transformation import etl_transform


class etl_execute:

    def __init__(self):
        # Setting up Spark and Glue for executing the Job
        print(f"{'*'*20} Setup Started for Spark... {'*'*20}")
        # self.args = getResolvedOptions(sys.argv, ["JOB_NAME"])

        # Uncomment below 3 lines if input is to be given via step functions
        self.args = getResolvedOptions(sys.argv, ["JOB_NAME", "source", "cdc_flag"])
        self.source_name = self.args["source"]
        self.cdc_flag = self.args["cdc_flag"]    
        

        # self.cdc_flag = "False"
        # self.source_name = "SCF_Salesforce"


        if (self.cdc_flag == "False") or (self.cdc_flag == "false"):
            self.cdc_flag = False
        elif (self.cdc_flag == "True") or (self.cdc_flag == "true"):
            self.cdc_flag = True
        print("Source Name: ",self.source_name)
        print("cdc_flag: ",self.cdc_flag)

        self.sc = SparkContext()
        self.glueContext = GlueContext(self.sc)
        self.spark = self.glueContext.spark_session
        self.job = Job(self.glueContext)
        self.job.init(self.args["JOB_NAME"], self.args)

        # Run the whole Framework
        self.execute()


    def execute(self):

        # Step 1: Loading the configuration file
        t1 = time.time()
        print(f"{'*'*20} Loading Config Data {'*'*20}")
        # self.config_data = utils.read_config("CC_SE_Salesforce")
        # self.config_data = utils.read_config("SCF_Salesforce")

        # Uncomment below line if input is to be given via step functions
        self.config_data = utils.read_config(self.source_name)

        print(f'Config File Loaded...')
        print(f"{'*'*50}")
        print('\n'*2)
        
        # Step 2: Loading the mapping file
        print(f"{'*'*20} Loading Mapping Data {'*'*20}")
        self.map_df = utils.read_map_file(self.config_data['mapping_file_name'])
        print(f'Mapping File Loaded...\n Mapping File ->', self.map_df)
        print(f"{'*'*50}")
        print('\n'*2)

        # Creating an instance of rds operations to perform RDS DB actions
        self.utils_obj = rds_ops(self.spark, self.config_data, self.map_df)
        
        # Step 3: Establishing connection to the PostgreSQL DB
        # Connection is made is this script and not utils because there will be multiple connection initiations if it was in utils
        print(f"{'*'*20} Entering the connection script... {'*'*20}")
        self.connection = self.utils_obj.get_rds_connection()
        self.cursor = self.connection.cursor()
        
        # Step 4: Fetching all tables from landing for a specific source provided in the configuration file
        t2 = time.time()
        print(f"{'*'*20} Time taken for loading mapping & config files : {(t2-t1) * 10**3}ms {'*'*20}")
        print(f"{'*'*20} Loading Landing Tables {'*'*20}")
        self.land_tables = self.utils_obj.etl_extraction(self.cdc_flag)
        # self.land_tables = {}
        print('Landing DataFrames Loaded... Number of Data Frames : {} '.format(len(self.land_tables)))
        print(f"{'*'*50}")
        print('\n'*2)
        
        # Step 5: Execution of the ETL tranformation and mapping script
        t3 = time.time()
        print(f"{'*'*20} Time taken for loading landing tables : {(t3-t2) * 10**3}ms {'*'*20}")
        self.pdm_transform = etl_transform(self.config_data, self.map_df, self.spark, self.glueContext, self.land_tables)
        self.output = self.pdm_transform.transformation()
        print(f"{'*'*20} ETL Transformation completed {'*'*20}")
        print('Transformed DataFrames Loaded... Number of Data Frames after transformation : {} '.format(len(self.output)))
        print(f"{'*'*50}")
        print('\n'*2)

        # Step 6: Exporting all the transformed tables to the staging DB
        t4 = time.time()
        print(f"{'*'*20} Time taken for transformation of all tables : {(t4-t3) * 10**3}ms {'*'*20}")
        print(f"{'*'*20} Exporting Data to PostgreSQL DB {'*'*20}")
        for table_name, df_data_list in self.output.items():
            print(f"Export to PostgreSQL started for table : {table_name}")
            key = self.map_df.loc[(self.map_df['TARGET_TABLE'] == table_name) & (self.map_df['IS_KEY'] == 'y')]['TARGET_COLUMN'].tolist()[0]
            print("key:", key)
            self.utils_obj.export_to_postgresql(self.connection, self.cursor, self.config_data["export_schema"], table_name, df_data_list, key)
            # for idx, batch_data in enumerate(df_data_list,1):
            #     self.utils_obj.export_to_postgresql(self.connection, self.cursor, self.config_data["export_schema"], table_name, batch_data, key)
            #     print(f"{'*'*20} Exporting Data to PostgreSQL DB completed for batch {idx}/{len(df_data_list)} {'*'*20}")           
            print(f"Export to PostgreSQL successful for table : {table_name}")
        print('Staging tables loaded... Number of tables : {} '.format(len(self.output)))
        print(f"{'*'*50}")
        print('\n'*2)    
        t5 = time.time()
        print(f"{'*'*20} Time taken for exporting the tables into PostgreSQL DB : {(t5-t4) * 10**3}ms {'*'*20}")        
        
        
        # Committing the Job
        self.job.commit()
        # Closing the cursor and the connection
        self.cursor.close()
        self.connection.close()


"""**************************************************Execution starts here*****************************************************"""        

#Execute and compute the time needed for the code to run
start_time = time.time()
print(f"{'*'*50} Data ETL Started {'*'*50}")
etl_execute()
print(f"{'*'*50} Data ETL Process Ended in {time.time()-start_time} seconds {'*'*50}")

"""*****************************************************************************************************************************"""
