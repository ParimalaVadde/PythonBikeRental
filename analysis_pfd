Description
As a developer, understand how data is coming from UT and create a design document on how an this be integrated with our existing data pipeline.

Tasks:

1. Ask Andy team to share a sample UT file

2. Understand file formats, data structure

3. Finalize on how will the files be received
4. create a design document to load to sds with existing data pipeline

5. Update confluence 

6. Prod fix in PFD pipeline if occurred

 

Acceptance Criteria:

Design document on how to consume UT data and integrate in our data pipeline


****************************************************************************************************************************************************************************************************************


1.case creation Countinous recuritment Jikesh 
2.Enigma contact load Jikesh
3.integrate the api for the continous recruitment Poorna
4.Continous recruitment ui page - Nishant
 
salesforce integration (Ramsai,Srinivas, ANusha,Rakesh,Sri)
1. ddl for recruitment activity,outcome,payment coordinates and also any model changes in the existing tables
2. after ddl creation we need to run end to end testing(PFD,Upload, salesforce)
3.Payment coordinates we need to write logic
4.recruitment activity & outcome & payment coordinates - upsert we need to load this to main schema
5.contacts,restrictions, telecommunication, address, electronic, characteristics --insert in the staging schema
6.integrate this glue job to the step function
7.testing & data validation in main schema 
8.event bridge creation to run the salesforce in a scheduled time
9. Model changes if any in the salesforce integration
10. Full end to end testing (PFD,Upload, salesforce Ingestion & extraction)
 
Ip data load - Abhishek
 
UT data load to sds (Mounika B, Parimala)
 
Ao Comms: (Mounika G, )
PQC Phase-1 (Confidentiality) Transition Planning for 2026
 
Farm:


************************************************************************

Here are **two clear, well-defined Jira sub-tasks** you can create under that story:

---

### **Sub-task 1: Analyze UT Data and Define Integration Approach**

**Description:**

* Request and review a sample UT file from Andyâ€™s team.
* Analyze the file format, schema, and data structure.
* Determine how the UT files will be received (frequency, location, and naming conventions).
* Draft the initial approach for integrating UT data with the existing data pipeline.

**Deliverables:**

* Summary of UT data structure and ingestion method.
* Proposed integration approach documented.

**Acceptance Criteria:**

* UT file structure and data flow are clearly understood.
* Integration approach is agreed upon by the team.

---

### **Sub-task 2: Create and Document UT Data Integration Design**

**Description:**

* Create a detailed design document outlining how UT data will be loaded into SDS through the existing data pipeline.
* Include transformations, mapping logic, and any required updates to the PFD pipeline.
* Update Confluence with the finalized design and handle any identified production fixes if necessary.

**Deliverables:**

* Final design document for UT data integration.
* Updated Confluence page with design details.

**Acceptance Criteria:**

* Design document reviewed and approved.
* Confluence updated and any necessary pipeline fixes applied.

---

Would you like me to format them in **Jira Markdown** (so you can copy-paste directly into Jira)?




Perform end-to-end Salesforce integration testing
