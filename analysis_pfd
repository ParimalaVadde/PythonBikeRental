Description
As a developer, understand how data is coming from UT and create a design document on how an this be integrated with our existing data pipeline.

Tasks:

1. Ask Andy team to share a sample UT file

2. Understand file formats, data structure

3. Finalize on how will the files be received
4. create a design document to load to sds with existing data pipeline

5. Update confluence 

6. Prod fix in PFD pipeline if occurred

 

Acceptance Criteria:

Design document on how to consume UT data and integrate in our data pipeline


****************************************************************************************************************************************************************************************************************


1.case creation Countinous recuritment Jikesh 
2.Enigma contact load Jikesh
3.integrate the api for the continous recruitment Poorna
4.Continous recruitment ui page - Nishant
 
salesforce integration (Ramsai,Srinivas, ANusha,Rakesh,Sri)
1. ddl for recruitment activity,outcome,payment coordinates and also any model changes in the existing tables
2. after ddl creation we need to run end to end testing(PFD,Upload, salesforce)
3.Payment coordinates we need to write logic
4.recruitment activity & outcome & payment coordinates - upsert we need to load this to main schema
5.contacts,restrictions, telecommunication, address, electronic, characteristics --insert in the staging schema
6.integrate this glue job to the step function
7.testing & data validation in main schema 
8.event bridge creation to run the salesforce in a scheduled time
9. Model changes if any in the salesforce integration
10. Full end to end testing (PFD,Upload, salesforce Ingestion & extraction)
 
Ip data load - Abhishek
 
UT data load to sds (Mounika B, Parimala)
 
Ao Comms: (Mounika G, )
PQC Phase-1 (Confidentiality) Transition Planning for 2026
 
Farm:
