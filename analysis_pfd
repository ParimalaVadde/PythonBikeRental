from pyspark.sql import DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
import ast

def flatten_nested_json_column(df, column_name, schema, explode_array=True):
    """
    Flattens a nested JSON column in a PySpark DataFrame.
    
    Args:
        df (DataFrame): The input PySpark DataFrame.
        column_name (str): The name of the JSON column to flatten.
        schema (StructType or ArrayType): The schema of the JSON column.
        explode_array (bool): Whether to explode arrays or join with semicolon.
    
    Returns:
        DataFrame: The DataFrame with the flattened JSON column.
    """
    try:
        # Check if the column exists
        if column_name not in df.columns:
            print(f"Column {column_name} does not exist in DataFrame")
            return df
            
        # Check if all values in the column are null
        non_null_count = df.filter(col(column_name).isNotNull()).count()
        
        if non_null_count == 0:
            print(f"Column {column_name} contains only null values, skipping flattening")
            return df
        
        # Parse the JSON column into structured data
        df = df.withColumn(column_name, from_json(col(column_name), schema))
        
        # Handle based on schema type
        if isinstance(schema, ArrayType):
            if not explode_array:
                # Join array elements with semicolon for non-exploded arrays
                df = df.withColumn(column_name, concat_ws(";", col(column_name)))
                return df
            
            # Create a row number to track original records
            df = df.withColumn("_row_id", monotonically_increasing_id())
            
            # Explode the array, keeping nulls
            df_exploded = df.select("*", explode(when(col(column_name).isNotNull(), col(column_name)).otherwise(array(lit(None).cast(schema.elementType)))).alias(f"{column_name}_exploded"))
            
            # Flatten the exploded column
            if isinstance(schema.elementType, StructType):
                for field in schema.elementType.fields:
                    field_name = field.name
                    field_type = field.dataType
                    
                    if isinstance(field_type, StructType):
                        # Handle nested structs
                        for nested_field in field_type.fields:
                            nested_field_name = nested_field.name
                            df_exploded = df_exploded.withColumn(
                                f"{column_name}__{field_name}__{nested_field_name}",
                                col(f"{column_name}_exploded.{field_name}.{nested_field_name}")
                            )
                    else:
                        # Handle simple fields
                        df_exploded = df_exploded.withColumn(
                            f"{column_name}__{field_name}", 
                            col(f"{column_name}_exploded.{field_name}")
                        )
            
            # Drop helper columns
            df_exploded = df_exploded.drop(column_name, f"{column_name}_exploded", "_row_id")
            return df_exploded
            
        elif isinstance(schema, StructType):
            # Handle single struct (not array)
            for field in schema.fields:
                field_name = field.name
                field_type = field.dataType
                
                if isinstance(field_type, StructType):
                    # Handle nested structs
                    for nested_field in field_type.fields:
                        nested_field_name = nested_field.name
                        df = df.withColumn(
                            f"{column_name}__{field_name}__{nested_field_name}",
                            col(f"{column_name}.{field_name}.{nested_field_name}")
                        )
                else:
                    # Handle simple fields
                    df = df.withColumn(
                        f"{column_name}__{field_name}", 
                        col(f"{column_name}.{field_name}")
                    )
            
            # Drop the original JSON column
            df = df.drop(column_name)
            return df
            
    except Exception as error:
        print(f"Error flattening column {column_name}: {error}")
        print(f"Schema type: {type(schema)}")
        # Return original dataframe on error
        return df
    
    return df

# Updated schemas based on your sample data
# For card_revenue - this appears to be a single object, not an array
card_revenue_schema = StructType([
    StructField("end_date", StringType(), True),
    StructField("date_accessible", StringType(), True),  # Added missing field
    StructField("1m", StructType([
        StructField("start_date", StringType(), True),
        StructField("average_monthly_amount", DoubleType(), True)
    ]), True),
    StructField("3m", StructType([
        StructField("start_date", StringType(), True),
        StructField("average_monthly_amount", DoubleType(), True)
    ]), True),
    StructField("12m", StructType([
        StructField("start_date", StringType(), True),
        StructField("average_monthly_amount", DoubleType(), True)
    ]), True)
])

# For card_transactions_stability - single object
card_transactions_stability_schema = StructType([
    StructField("end_date", StringType(), True),
    StructField("date_accessible", StringType(), True),  # Added missing field
    StructField("1m", StructType([
        StructField("start_date", StringType(), True),
        StructField("days_present", IntegerType(), True),
        StructField("weeks_present", IntegerType(), True),
        StructField("months_present", IntegerType(), True),
        StructField("daily_coverage_ratio", DoubleType(), True),
        StructField("weekly_coverage_ratio", DoubleType(), True),
        StructField("monthly_coverage_ratio", DoubleType(), True)
    ]), True),
    StructField("3m", StructType([
        StructField("start_date", StringType(), True),
        StructField("days_present", IntegerType(), True),
        StructField("weeks_present", IntegerType(), True),
        StructField("months_present", IntegerType(), True),
        StructField("daily_coverage_ratio", DoubleType(), True),
        StructField("weekly_coverage_ratio", DoubleType(), True),
        StructField("monthly_coverage_ratio", DoubleType(), True)
    ]), True),
    StructField("12m", StructType([
        StructField("start_date", StringType(), True),
        StructField("days_present", IntegerType(), True),
        StructField("weeks_present", IntegerType(), True),
        StructField("months_present", IntegerType(), True),
        StructField("daily_coverage_ratio", DoubleType(), True),
        StructField("weekly_coverage_ratio", DoubleType(), True),
        StructField("monthly_coverage_ratio", DoubleType(), True)
    ]), True)
])

# For associated_people - this is an array
associated_people_schema = ArrayType(StructType([
    StructField("name", StringType(), True),
    StructField("titles", ArrayType(StringType()), True)
]))

# For industries - this appears to be a single object based on sample
industries_schema = StructType([
    StructField("classification_type", StringType(), True),
    StructField("classification_code", StringType(), True),
    StructField("classification_description", StringType(), True)
])

# For technologies - this appears to be a single object based on sample  
technologies_schema = StructType([
    StructField("vendor_name", StringType(), True),
    StructField("category", StringType(), True)
])

def flatten_associated_people_with_titles(df, column_name="associated_people"):
    """
    Special function to handle associated_people which has nested arrays (titles within each person)
    """
    try:
        if column_name not in df.columns:
            print(f"Column {column_name} does not exist in DataFrame")
            return df
            
        # Check for non-null values
        non_null_count = df.filter(col(column_name).isNotNull()).count()
        if non_null_count == 0:
            print(f"Column {column_name} contains only null values, skipping flattening")
            return df
        
        # Parse JSON
        df = df.withColumn(column_name, from_json(col(column_name), associated_people_schema))
        
        # Add row ID to track original records
        df = df.withColumn("_row_id", monotonically_increasing_id())
        
        # First explode the people array
        df_people = df.select("*", explode(when(col(column_name).isNotNull(), col(column_name)).otherwise(array(lit(None).cast(associated_people_schema.elementType)))).alias("person"))
        
        # Extract name and titles
        df_people = df_people.withColumn("person_name", col("person.name"))
        df_people = df_people.withColumn("person_titles", col("person.titles"))
        
        # Now explode the titles array for each person
        df_final = df_people.select("*", explode(when(col("person_titles").isNotNull(), col("person_titles")).otherwise(array(lit(None).cast(StringType())))).alias("title"))
        
        # Create final flattened columns
        df_final = df_final.withColumn(f"{column_name}__name", col("person_name"))
        df_final = df_final.withColumn(f"{column_name}__title", col("title"))
        
        # Clean up helper columns
        df_final = df_final.drop(column_name, "person", "person_name", "person_titles", "title", "_row_id")
        
        return df_final
        
    except Exception as error:
        print(f"Error flattening associated_people: {error}")
        return df

# Usage example:
"""
# Updated flattening logic
for column, schema in [
    ("card_revenue", card_revenue_schema),
    ("card_transactions_stability", card_transactions_stability_schema),
    ("industries", industries_schema),
    ("technologies", technologies_schema),
]:
    transformed_json_df = flatten_nested_json_column(transformed_json_df, column, schema)

# Special handling for associated_people due to nested arrays
transformed_json_df = flatten_associated_people_with_titles(transformed_json_df, "associated_people")
"""
