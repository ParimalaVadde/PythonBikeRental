def cast_spark_to_pandas_schema(spark_df):
    # Replace nulls in timestamp columns with a default value (e.g., pd.NaT or a specific date)
    timestamp_columns = [column for column, dtype in spark_df.dtypes if dtype == "timestamp"]
    integer_columns = [column for column, dtype in spark_df.dtypes if dtype == "int"]
    boolean_columns = [column for column, dtype in spark_df.dtypes if dtype == "boolean"]  # ADD THIS LINE

    # check the decimal columns values
    for field in spark_df.schema.fields:
        if isinstance(field.dataType, DecimalType) and field.dataType.scale == 18:
            print(f"{field} is of decimal type")
            spark_df = spark_df.withColumn(field.name, spark_df[field.name].cast(DecimalType(38, 2)))

    # Convert Spark DataFrame to pandas DataFrame
    pandas_df = spark_df.toPandas()

    # Explicitly set datetime64[ns] for timestamp columns
    for t_col in timestamp_columns:
        pandas_df[t_col] = pandas_df[t_col].astype("datetime64[ns]", copy=False)

    # Handle nullable integer columns (Int64)
    for column in integer_columns:
        pandas_df[column] = pandas_df[column].astype("Int64", copy=False)
        pandas_df[column] = pandas_df[column].replace(np.nan, None)
    
    # ADD THIS SECTION: Handle nullable boolean columns
    for column in boolean_columns:
        # Convert to nullable boolean dtype that supports NA/None
        pandas_df[column] = pandas_df[column].astype("boolean", copy=False)
        # Replace pd.NA with None for consistency
        pandas_df[column] = pandas_df[column].apply(lambda x: None if pd.isna(x) else x)

    # Replace NaT with None for datetime columns
    datetime_columns = pandas_df.select_dtypes(include=["datetime64[ns]"]).columns
    for column in datetime_columns:
        pandas_df[column] = pandas_df[column].apply(lambda x: None if pd.isna(x) else x)

    return pandas_df
