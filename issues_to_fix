def write_single_csv_to_s3_fixed(df: DataFrame, s3_bucket: str, s3_key: str, temp_path: str):
    """
    Writes a Spark DataFrame as a single CSV file with proper escaping to S3.
    Handles void data type columns by converting them to string type.
    """
    
    # Check for void columns and convert them to string type
    void_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, NullType)]
    
    if void_columns:
        print(f"Found void columns: {void_columns}. Converting to string type.")
        # Convert void columns to string type with null values
        for col_name in void_columns:
            df = df.withColumn(col_name, lit(None).cast("string"))
    
    # Alternative approach: Drop void columns entirely (uncomment if you prefer this)
    # if void_columns:
    #     print(f"Dropping void columns: {void_columns}")
    #     df = df.drop(*void_columns)
    
    try:
        # Ensure temp_path ends with a trailing slash
        if not temp_path.endswith('/'):
            temp_path += '/'
        
        # Coalesce to one partition and write to temp S3 directory with proper CSV options
        df.coalesce(1).write.mode("overwrite") \
            .option("header", "true") \
            .option("quote", '"') \
            .option("escape", '"') \
            .option("quoteAll", "true") \
            .option("multiLine", "true") \
            .option("timestampFormat", "yyyy-MM-dd HH:mm:ss") \
            .option("dateFormat", "yyyy-MM-dd") \
            .csv(temp_path)
        
        # Initialize S3 client
        s3 = boto3.client("s3")
        
        # Parse temp path
        temp_bucket = temp_path.replace("s3://", "").split("/")[0]
        temp_prefix = "/".join(temp_path.replace("s3://", "").split("/")[1:])
        
        # Remove trailing slash from prefix if present
        if temp_prefix.endswith('/'):
            temp_prefix = temp_prefix[:-1]
        
        # List objects in temp directory
        response = s3.list_objects_v2(Bucket=temp_bucket, Prefix=temp_prefix)
        
        if 'Contents' not in response:
            raise Exception(f"No files found in temp path: {temp_path}")
        
        objects = response['Contents']
        
        # Find the CSV part file, excluding _SUCCESS and other metadata files
        csv_files = [
            obj['Key'] for obj in objects 
            if obj['Key'].endswith(".csv") and not obj['Key'].endswith("_SUCCESS")
        ]
        
        if not csv_files:
            raise Exception(f"No CSV files found in temp directory: {temp_path}")
        
        # Use the first (and should be only) CSV file
        part_file = csv_files[0]
        
        # Copy the CSV file to the target location
        s3.copy_object(
            Bucket=s3_bucket,
            CopySource={'Bucket': temp_bucket, 'Key': part_file},
            Key=s3_key
        )
        
        print(f"Successfully copied CSV to s3://{s3_bucket}/{s3_key}")
        
        # Clean up temp files - delete all objects in the temp directory
        delete_objects = [{'Key': obj['Key']} for obj in objects]
        if delete_objects:
            s3.delete_objects(
                Bucket=temp_bucket,
                Delete={'Objects': delete_objects}
            )
            print(f"Cleaned up {len(delete_objects)} temp files")
            
    except Exception as e:
        print(f"Error in write_single_csv_to_s3_fixed: {str(e)}")
        raise
