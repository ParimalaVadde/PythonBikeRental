import sys
import boto3
import json
import hashlib
import uuid
import psycopg2
import utils as utl
import staging_schema as ss
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col, explode, lit, when, udf, to_date, current_date, from_json
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, IntegerType, MapType
from pyspark.sql.utils import AnalysisException
from datetime import datetime
from pyspark.sql.functions import  concat_ws,current_timestamp, broadcast
from pyspark.sql import DataFrame



## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)


s3 = boto3.client("s3")
#s3.download_file(s3_bucket,ssl_cert_s3_path,local_ssl_cert_path)


rds_client = boto3.client ("rds",region_name=ss.region)
rds_token = rds_client.generate_db_auth_token(
    DBHostname = ss.rds_host,
    Port = ss.rds_port,
    DBUsername = ss.db_user
    )
    
pg_connection_options = {
    "url" : f"jdbc:postgresql://{ss.rds_host}:{ss.rds_port}/{ss.db_name}?sslmode=require&sslrootcert={ss.ssl_cert_s3_path}",
    "user" : ss.db_user,
    "password" : rds_token
   # "db_table":source_table
    }

# Register the UDF
compute_uuid_udf = udf(utl.compute_uuid, StringType())


#truncate = utl.truncate_tables(ss.rds_host,ss.rds_port,ss.db_name,ss.db_user,rds_token,ss.ssl_cert_s3_path)

def get_landing_data():
    try:
        # Query to fetch data
        query = "(SELECT DISTINCT * FROM sds_landing.analysis_pfd WHERE filter_employee = 'N') AS temp_table"

        # Read the query result into a Spark DataFrame
        source_df = glueContext.read.format("jdbc").options(
                dbtable=query,
                **pg_connection_options
            ).load()

        # Define UDFs for computing business entity ID and buying entity ID
        
        

        source_df = source_df.withColumn(
                        "stg_business_entity_id",
                        when(
                            col("sds_supplier_id").isNotNull(), col("sds_supplier_id")
                            ).otherwise(
                                compute_uuid_udf(concat_ws(",", col("vendor_name_cleaned")))
                            )
                            )
                

                    
        print("stg_business_entity_id column added.")


        source_df = source_df.withColumn(
                "stg_buying_entity",
            compute_uuid_udf(concat_ws(",", col("buying_entity")))
                )
                
        print("stg_buying_entity column added.")

        print("After adding the UUID columns")
        source_df.select("stg_business_entity_id").show()
        # Return the Spark DataFrame
        return source_df

    except Exception as e:
        print('Error Message is:', e)
        return None
        


def process_mapping_and_transform_data(source_df, mapping_file_path, output_data_path):
    """
    Reads a mapping file and input data, applies column transformations, and writes the transformed data to S3.
    """
    # Read the mapping file into a Glue DynamicFrame
    mapping_df = glueContext.create_dynamic_frame.from_options(
        connection_type="s3",
        connection_options={"paths": [mapping_file_path]},
        format="csv",
        format_options={"withHeader": True}
    ).toDF()

    print("Reading mapping file completed")

    # Validate mapping DataFrame
    if mapping_df.rdd.isEmpty():
        raise ValueError("The mapping DataFrame is empty. Please check the mapping file.")
    if "source_column" not in mapping_df.columns or "target_column" not in mapping_df.columns:
        raise ValueError("The mapping DataFrame does not contain the required columns: 'source_column' and 'target_column'.")

    # Initialize variables
    column_mapping = {}
    current_date = datetime.today().strftime("%Y-%m-%d")

    # Iterate through the mapping DataFrame to create the column mapping dictionary
    for row in mapping_df.collect():
        source_col = str(row["source_column"]).strip().lower()
        target_col = row["target_column"]
        column_mapping[source_col] = target_col

   # print("Column Mapping Dictionary:")
   # print(column_mapping)

    # Read the input data into a Glue DynamicFrame
    source_df.show()
    data = source_df
    print("data:", data.show())

    # Validate input DataFrame
    if data.rdd.isEmpty():
      #  raise ValueError("The input DataFrame is empty. Please check the source data.")
      print("The input DataFrame is empty. Please check the source data.")

    # Verify that all source columns exist in the input DataFrame
    # missing_columns = [col for col in column_mapping.keys() if col not in data.columns]
    # if missing_columns:
    #     raise ValueError(f"The following columns are missing in the input data: {missing_columns}")
    #  #   print("The following columns are missing in the input data",missing_columns )
        
    print("before transforming the data")

    # Rename columns in the input DataFrame using the column mapping
    transformed_df = data.select(
        [col(c).alias(column_mapping[c]) if c in column_mapping else col(c) for c in data.columns]
    )
    # Add a new column "stg_jpmc_business_entity_id" based on the condition
    transformed_df = transformed_df.withColumn(
        "stg_jpmc_business_entity_id",
        when(
            (col("client_ecid").isNotNull()) | (col("ind_jpmc") == 'Y'),
            lit("2d35a04a-5fdf-50d5-7750-c1c7621ddc33")
        ).otherwise(None)
    )
    
    print("after transforming the data")

    print("Transformed DataFrame:")
    #transformed_df.show()

    # Validate transformed DataFrame
    if transformed_df.rdd.isEmpty():
        raise ValueError("The transformed DataFrame is empty after applying column mappings.")

    # Convert the Spark DataFrame to a Glue DynamicFrame
    #transformed_dynamic_frame = DynamicFrame.fromDF(transformed_df, glueContext, "transformed_dynamic_frame")


    

    print("Transformation and writing completed")
    return transformed_df
    
    
landing_tables= get_landing_data()
#landing_tables.show()
transformed_df = process_mapping_and_transform_data(landing_tables,ss.mapping_file_path, ss.output_data_path)


# Add these CSV writing options to handle special characters properly

def write_single_csv_to_s3_fixed(df: DataFrame, s3_bucket: str, s3_key: str, temp_path: str):
    """
    Writes a Spark DataFrame as a single CSV file with proper escaping to S3.
    """
    # Coalesce to one partition and write to temp S3 directory with proper CSV options
    df.coalesce(1).write.mode("overwrite") \
        .option("header", True) \
        .option("quote", '"') \
        .option("escape", '"') \
        .option("quoteAll", True) \
        .option("multiLine", True) \
        .csv(temp_path)

    # Rest of your S3 copy logic remains the same
    s3 = boto3.client("s3")
    temp_bucket = temp_path.replace("s3://", "").split("/")[0]
    temp_prefix = "/".join(temp_path.replace("s3://", "").split("/")[1:])

    objects = s3.list_objects_v2(Bucket=temp_bucket, Prefix=temp_prefix)
    part_file = next(
        obj['Key'] for obj in objects.get('Contents', []) if obj['Key'].endswith(".csv")
    )

    s3.copy_object(
        Bucket=s3_bucket,
        CopySource={'Bucket': temp_bucket, 'Key': part_file},
        Key=s3_key
    )

    # Clean up temp files
    for obj in objects.get('Contents', []):
        s3.delete_object(Bucket=temp_bucket, Key=obj['Key'])



def transform_dataframes(transformed_df, ss, glueContext, compute_uuid_udf, utl):
    """
    Function to transform multiple DataFrames based on the provided schemas and logic.

    Args:
        transformed_df (DataFrame): The input PySpark DataFrame.
        ss (object): An object containing schema definitions and column mappings.
        glueContext (GlueContext): The AWS Glue context.
        compute_uuid_udf (UDF): A UDF to compute UUIDs.
        utl (module): A utility module containing helper functions like `flatten_nested_json_column` and `melt_dataframe`.

    Returns:
        dict: A dictionary of transformed DataFrames.
    """
    # Flatten the JSON columns
    transformed_json_df = transformed_df.select(*ss.jsoncol)
 
    # Flatten nested JSON columns
    for column, schema in [
        ("card_revenue", ss.card_revenue_schema),
        ("card_transactions_stability", ss.card_transactions_stability),
        ("associated_people", ss.associated_people_schema),
        #("industries", ss.industries_schema),
        #("company_structure", ss.company_structure_schema),
        #("technologies", ss.technologies_schema),
    ]:
        transformed_json_df = utl.flatten_nested_json_column(transformed_json_df, column, schema)

    # Explode and process specific columns
    #transformed_json_df = transformed_json_df.withColumn("associated_people__title", explode(col("associated_people__titles"))).drop("associated_people__titles")
    #transformed_json_df = transformed_json_df.withColumn("associated_tax_ids", from_json(col("associated_tax_ids"), ss.associated_tax_ids_schema))
    #transformed_json_df = transformed_json_df.withColumn("associated_tax_id", explode(col("associated_tax_ids"))).drop("associated_tax_ids")
    #transformed_json_df = transformed_json_df.withColumn("registered_agents", from_json(col("registered_agents"), ss.registered_agents_schema))
    #transformed_json_df = transformed_json_df.withColumn("registered_agents", explode(col("registered_agents")))
    # transformed_json_df = utl.flatten_nested_json_column(
    #     transformed_json_df,
    #     "registered_agents",
    #     ss.registered_agents_schema,
    #     explode_array=False)

    transformed_json_df.show()
    
    
    return transformed_json_df
    


transformed_dataframes = transform_dataframes(transformed_df, ss, glueContext, compute_uuid_udf, utl)

# Use the fixed CSV writing function
write_single_csv_to_s3_fixed(
    transformed_dataframes,
    s3_bucket="app-id-111597-dep-id-114116-uu-id-by081rbjj1vo",
    s3_key="upsert/pfd_staging/transformed_json_df.csv",
    temp_path="s3://app-id-111597-dep-id-114116-uu-id-by081rbjj1vo/upsert/pfd_staging/_tmp"
)

job.commit()


*********************************************************************************************************************************


i m getting error for 
AnalysisException: CSV data source does not support array<string> data type.

associated_people in staging_schema.py

# existing Define the schema for the `associated_people` column
associated_people_schema =  ArrayType(StructType([
                StructField("name", StringType(), True),
                StructField("titles", ArrayType(StringType()), True)
            ]))

associated_people new file sample value

new file

[{'name': 'AARON UNDERWOOD', 'titles': ['CEO', 'CONTACT NAME', 'OWNER']}, {'name': 'TINA UNDERWOOD', 'titles': ['CFO', 'SECRETARY']}]


old file 

"[ {
            ""name"": ""VIRAT KOHLI"",
            ""titles"": [
            		""CEO"",
                ""OFFICER""
            ]
        },
        {
            ""name"": ""JADEJA RAVI"",
            ""titles"": [
               ""OFFICER""
            ]
        }]"

  
#function from utils file   
  
def flatten_nested_json_column(df, column_name, schema, explode_array=True):
    """
    Flattens a nested JSON column in a PySpark DataFrame.
    
    Args:
        df (DataFrame): The input PySpark DataFrame.
        column_name (str): The name of the JSON column to flatten.
        schema (StructType): The schema of the JSON column.
        explode_array (bool): Whether to explode arrays or join with semicolon.
    
    Returns:
        DataFrame: The DataFrame with the flattened JSON column.
    """
    try:
        # Parse the JSON column into structured data
        df = df.withColumn(column_name, from_json(col(column_name), schema))
        
        # If explode_array is False, join array elements with semicolon
        if not explode_array:
            df = df.withColumn(column_name, concat_ws(";", col(column_name)))
            return df
        
        # Explode the array to create one row per element
        df = df.withColumn(column_name, explode(col(column_name)))
        # Iterate over the fields in the schema
        for field in schema.elementType.fields:  # Use `elementType` for ArrayType
            field_name = field.name
            field_type = field.dataType
            # If the field is a nested struct, recursively flatten it
            if isinstance(field_type, StructType):
                for nested_field in field_type.fields:
                    nested_field_name = nested_field.name
                    df = df.withColumn(
                        f"{column_name}__{field_name}__{nested_field_name}",
                        col(f"{column_name}.{field_name}.{nested_field_name}")
                    )
            else:
                # If the field is not nested, extract it directly
                df = df.withColumn(f"{column_name}__{field_name}", col(f"{column_name}.{field_name}"))
        # Drop the original JSON column
        df = df.drop(column_name)
    except Exception as error:
        print(f"Error flattening column {column_name}: {error}")
    return df



else:
    if isinstance(field_type, ArrayType) and isinstance(field_type.elementType, StringType):
        # Join array<string> with semicolon
        df = df.withColumn(f"{column_name}__{field_name}", concat_ws(";", col(f"{column_name}.{field_name}")))
    else:
        df = df.withColumn(f"{column_name}__{field_name}", col(f"{column_name}.{field_name}"))
