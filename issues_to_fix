def cast_spark_to_pandas_schema(spark_df):
    # Identify column types
    timestamp_columns = [column for column, dtype in spark_df.dtypes if dtype == "timestamp"]
    integer_columns = [column for column, dtype in spark_df.dtypes if dtype == "int"]
    boolean_columns = [column for column, dtype in spark_df.dtypes if dtype == "boolean"]
    
    # Check the decimal columns values
    for field in spark_df.schema.fields:
        if isinstance(field.dataType, DecimalType) and field.dataType.scale == 18:
            print(f"{field} is of decimal type")
            spark_df = spark_df.withColumn(field.name, spark_df[field.name].cast(DecimalType(38, 2)))

    # Convert Spark DataFrame to pandas DataFrame
    pandas_df = spark_df.toPandas()

    # Handle timestamp columns
    for t_col in timestamp_columns:
        pandas_df[t_col] = pandas_df[t_col].astype("datetime64[ns]", copy=False)

    # Handle nullable integer columns
    for column in integer_columns:
        pandas_df[column] = pandas_df[column].astype("Int64", copy=False)
        pandas_df[column] = pandas_df[column].replace(np.nan, None)
    
    # Handle boolean columns - PRESERVE NULL as None, not False
    for column in boolean_columns:
        # Keep as object type to properly support None
        pandas_df[column] = pandas_df[column].apply(
            lambda x: None if pd.isna(x) or x is None else bool(x)
        )

    # Replace NaT with None for datetime columns
    datetime_columns = pandas_df.select_dtypes(include=["datetime64[ns]"]).columns
    for column in datetime_columns:
        pandas_df[column] = pandas_df[column].apply(lambda x: None if pd.isna(x) else x)

    return pandas_df
