import json
import hashlib
import uuid
import psycopg2
import utils as utl
import staging_schema as ss
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col, explode, lit, when, udf, to_date, current_date, from_json, struct, regexp_replace
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, IntegerType, MapType
from pyspark.sql.utils import AnalysisException
from datetime import datetime
from pyspark.sql.functions import concat_ws,current_timestamp,broadcast,round,size
from pyspark.sql import DataFrame
from functools import reduce
from pyspark.sql.functions import col, lit, when, isnan, isnull
from pyspark.sql.types import NullType
from pyspark.sql.functions import lit
import sys
import boto3


## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)


s3 = boto3.client("s3")
#s3.download_file(s3_bucket,ssl_cert_s3_path,local_ssl_cert_path)



# Register the UDF
compute_uuid_udf = udf(lambda x: utl.compute_uuid(x, ss.decimal_columns), StringType())


csv_file_path = 's3://app-id-111597-dep-id-114116-uu-id-9x0jt94siuto/pfd_scripts/pfd_staging_pvr_test/transformed_df.csv'

transformed_df = spark.read \
                 .option("header","true") \
                 .option("inferSchema","true") \
                 .csv(csv_file_path)
                 
transformed_df.show()


def write_single_csv_to_s3_fixed(df: DataFrame, s3_bucket: str, s3_key: str, temp_path: str):
    """
    Writes a Spark DataFrame as a single CSV file with proper escaping to S3.
    Handles void data type columns by converting them to string type.
    """
    
    # Check for void columns and convert them to string type
    void_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, NullType)]
    
    if void_columns:
        print(f"Found void columns: {void_columns}. Converting to string type.")
        # Convert void columns to string type with null values
        for col_name in void_columns:
            df = df.withColumn(col_name, lit(None).cast("string"))
    
    # Alternative approach: Drop void columns entirely (uncomment if you prefer this)
    # if void_columns:
    #     print(f"Dropping void columns: {void_columns}")
    #     df = df.drop(*void_columns)
    
    try:
        # Ensure temp_path ends with a trailing slash
        if not temp_path.endswith('/'):
            temp_path += '/'
        
        # Coalesce to one partition and write to temp S3 directory with proper CSV options
        df.coalesce(1).write.mode("overwrite") \
            .option("header", "true") \
            .option("quote", '"') \
            .option("escape", '"') \
            .option("quoteAll", "true") \
            .option("multiLine", "true") \
            .option("timestampFormat", "yyyy-MM-dd HH:mm:ss") \
            .option("dateFormat", "yyyy-MM-dd") \
            .csv(temp_path)
        
        # Initialize S3 client
        s3 = boto3.client("s3")
        
        # Parse temp path
        temp_bucket = temp_path.replace("s3://", "").split("/")[0]
        temp_prefix = "/".join(temp_path.replace("s3://", "").split("/")[1:])
        
        # Remove trailing slash from prefix if present
        if temp_prefix.endswith('/'):
            temp_prefix = temp_prefix[:-1]
        
        # List objects in temp directory
        response = s3.list_objects_v2(Bucket=temp_bucket, Prefix=temp_prefix)
        
        if 'Contents' not in response:
            raise Exception(f"No files found in temp path: {temp_path}")
        
        objects = response['Contents']
        
        # Find the CSV part file, excluding _SUCCESS and other metadata files
        csv_files = [
            obj['Key'] for obj in objects 
            if obj['Key'].endswith(".csv") and not obj['Key'].endswith("_SUCCESS")
        ]
        
        if not csv_files:
            raise Exception(f"No CSV files found in temp directory: {temp_path}")
        
        # Use the first (and should be only) CSV file
        part_file = csv_files[0]
        
        # Copy the CSV file to the target location
        s3.copy_object(
            Bucket=s3_bucket,
            CopySource={'Bucket': temp_bucket, 'Key': part_file},
            Key=s3_key
        )
        
        print(f"Successfully copied CSV to s3://{s3_bucket}/{s3_key}")
        
        # Clean up temp files - delete all objects in the temp directory
        delete_objects = [{'Key': obj['Key']} for obj in objects]
        if delete_objects:
            s3.delete_objects(
                Bucket=temp_bucket,
                Delete={'Objects': delete_objects}
            )
            print(f"Cleaned up {len(delete_objects)} temp files")
            
    except Exception as e:
        print(f"Error in write_single_csv_to_s3_fixed: {str(e)}")
        raise



# Flatten the JSON columns
jsoncol = ['stg_business_entity_id','card_revenue']
transformed_json_df = transformed_df.select(jsoncol)

# Process all JSON columns
flattened_dataframes = utl.process_all_json_columns(transformed_json_df)

# For columns that need to be kept in main table (like registered_agents)
# Handle them separately:
if 'registered_agents' in transformed_json_df.columns:
    transformed_json_df = transformed_json_df.withColumn(
        "registered_agents",
        when(
            ~utl.is_empty_or_null("registered_agents"),
            concat_ws(";", from_json(col("registered_agents"), ArrayType(StringType())))
        ).otherwise(lit(None))
    )

# Combine everything
# final_df = utl.combine_flattened_results(
#     transformed_json_df.select("stg_business_entity_id", "registered_agents"), 
#     flattened_dataframes
# )

final_df = utl.combine_flattened_results(
    transformed_json_df.select("stg_business_entity_id"), 
    flattened_dataframes
)
print("*********************",flattened_dataframes.columns)
# Use the fixed CSV writing function
write_single_csv_to_s3_fixed(
    final_df,
    s3_bucket="app-id-111597-dep-id-114116-uu-id-9x0jt94siuto",
    s3_key="pfd_scripts/pfd_staging_pvr_test/final_df.csv",
    temp_path="s3://app-id-111597-dep-id-114116-uu-id-9x0jt94siuto/upsert/pfd_staging/_tmp"
)

job.commit()


Error Category: QUERY_ERROR; Failed Line Number: 178; AnalysisException: Found duplicate column(s) when inserting into s3://app-id-111597-dep-id-114116-uu-id-9x0jt94siuto/upsert/pfd_staging/_tmp: `card_revenue`
