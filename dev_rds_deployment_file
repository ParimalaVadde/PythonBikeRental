import sys
import os
import boto3
import json
import hashlib
import uuid
import psycopg2
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col, lit, current_date, current_timestamp, struct
from pyspark.sql.types import StringType
from functools import reduce
 
## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
 
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
 
region = "us-east-1"
rds_host = "rdsapg86a8-auroratdb-v1-node-1.chzdhpfung6p.us-east-1.rds.amazonaws.com"
rds_port = 6160
db_name = "xbsddevdb"
db_user = "xbsddevdbAuroraAppAdmin"
ssl_cert_s3_path = "s3://app-id-111597-dep-id-114116-uu-id-9x0jt94siuto/cert/us-east-1-bundle.pem"
 
rds_client = boto3.client("rds", region_name=region)
rds_token = rds_client.generate_db_auth_token(
    DBHostname=rds_host,
    Port=rds_port,
    DBUsername=db_user
)
 
pg_connection_options = {
    "url": f"jdbc:postgresql://{rds_host}:{rds_port}/{db_name}?sslmode=require&sslrootcert={ssl_cert_s3_path}",
    "user": db_user,
    "password": rds_token
}
 
# UDF to compute UUID from concatenated column values
def compute_uuid(data_row, decimal_columns):
    values = []
    data = data_row.asDict()
    for column in sorted(data.keys()):
        val = data[column]
        if val is not None and str(val).strip().lower() not in {"", "nan", "null", "none", "nat"}:
            if column in decimal_columns:
                val = f"{float(val):.6f}"
            values.append(str(val).strip().lower())
    name = ','.join(values)
    return str(uuid.UUID(bytes=hashlib.sha256(name.encode()).digest()[:16])) if name else None
 
decimal_columns = [
    "sum_of_payments", "chargeback_amount", "chargeback_percentage", "payment_terms_discount_rate",
    "card_revenue__1m__average_monthly_amount", "card_revenue__3m__average_monthly_amount",
    "card_revenue__12m__average_monthly_amount", "card_transactions_stability__1m__daily_coverage_ratio",
    "card_transactions_stability__1m__weekly_coverage_ratio", "card_transactions_stability__1m__monthly_coverage_ratio",
    "card_transactions_stability__3m__daily_coverage_ratio", "card_transactions_stability__3m__weekly_coverage_ratio",
    "card_transactions_stability__3m__monthly_coverage_ratio", "card_transactions_stability__12m__daily_coverage_ratio",
    "card_transactions_stability__12m__weekly_coverage_ratio", "card_transactions_stability__12m__monthly_coverage_ratio"
]
 
compute_uuid_udf = spark.udf.register("compute_uuid_udf", lambda x: compute_uuid(x, decimal_columns), StringType())
 
def get_landing_data():
    try:
        select_columns = [
            'bed."business_entity_id"',
            'bed."business_entity_name"',
            'act."Name"',
            'act."BillingStreet"',
            'act."BillingCity"',
            'act."BillingState"',
            'act."BillingPostalCode"',
            'act."BillingCountry"',
            'act."ShippingStreet"',
            'act."ShippingCity"',
            'act."ShippingState"',
            'act."ShippingPostalCode"',
            'act."ShippingCountry"',
            'cs."Remit_Street_1__c"',
		    'cs."Remit_Street_2__c"',
	    	'cs."Remit_City__c"',
	    	'cs."Remit_State_Province__c"',
	    	'cs."Remit_Zip_Code_Postal_Code__c"',
	    	'cs."Remit_Country__c"',
	    	'cnt."MailingStreet"',
		    'cnt."MailingCity"',
	    	'cnt."MailingState"',
		    'cnt."MailingPostalCode"',
		    'cnt."MailingCountry"'
        ]
 
        address_types = [
            {
                "type": "billing",
                "fields": {
                    "street_line_1": "BillingStreet",
                    "country": "BillingCountry",
                    "city": "BillingCity",
                    "state_province": "BillingState",
                    "postal_code": "BillingPostalCode"
                }
            },
            {
                "type": "shipping",
                "fields": {
                    "street_line_1": "ShippingStreet",
                    "country": "ShippingCountry",
                    "city": "ShippingCity",
                    "state_province": "ShippingState",
                    "postal_code": "ShippingPostalCode"
                }
            },
            {
                "type": "remittance",
                "fields": {
                    "street_line_1": "Remit_Street_1__c",
                    "street_line_2": "Remit_Street_2__c",
                    "country": "Remit_Country__c",
                    "city": "Remit_City__c",
                    "state_province": "Remit_State_Province__c",
                    "postal_code": "Remit_Zip_Code_Postal_Code__c"
                }
            },
            {
                "type": "mailing",
                "fields": {
                    "street_line_1": "MailingStreet",
                    "country": "MailingCountry",
                    "city": "MailingCity",
                    "state_province": "MailingState",
                    "postal_code": "MailingPostalCode"
                }
            }
         ]
 
        query = f""" (SELECT DISTINCT  {', '.join(select_columns)} \
        FROM sds_landing.account_ccpresales act \
        LEFT JOIN sds_landing.case_ccpresales cs ON act."Id" = cs."AccountId" \
        LEFT JOIN sds_landing.contact_ccpresales cnt ON act."Id" = cnt."AccountId" \
        LEFT JOIN sds_landing.suppliercampaignphase__c_ccpresales scp ON act."Id" = scp."Client_Account__c" \
        INNER JOIN sds_main.business_entity_details bed ON lower(act."Name") = lower(bed."business_entity_name")) AS temp_table"""


print("****query read is completed****")
 
        source_df = glueContext.read.format("jdbc").options(
            dbtable=query,
            **pg_connection_options
        ).load()
        print("****data read completed****")
 
        # Collect all needed source columns for select
        print("before address types....")
        all_fields = set(["business_entity_id"])
        for addr in address_types:
            all_fields.update(addr["fields"].values())
        account_df = source_df.select([col(f) for f in all_fields])
        print("before addres types condtion...")
        address_dfs = []
        for addr in address_types:
            not_null_conditions = [
                (col(field) != "") & col(field).isNotNull()
                for field in addr["fields"].values()
            ]
            address_condition = not_null_conditions[0]
            for cond in not_null_conditions[1:]:
                address_condition = address_condition | cond
            # Handle street_line_2: use actual column if present, else None
            street_line_2_col = addr["fields"].get("street_line_2", None)
            if street_line_2_col:
                street_line_2_expr = col(street_line_2_col).alias("street_line_2")
            else:
                street_line_2_expr = lit(None).alias("street_line_2")
            # Similarly for street_line_3 if needed
            street_line_3_col = addr["fields"].get("street_line_3", None)
            if street_line_3_col:
                street_line_3_expr = col(street_line_3_col).alias("street_line_3")
            else:
                street_line_3_expr = lit(None).alias("street_line_3")    
            related_identifier_source_value = "contacts" if addr["type"] == "mailing" else "business_entity"    
            print("before select_exprs ...")
            select_exprs = [
                col("business_entity_id").alias("related_identifier"),
                # lit(related_identifier_source_value).alias("related_identifier_source"),
                lit("business_entity").alias("related_identifier_source"),
                lit(addr["type"]).alias("physical_address_type"),
                col(addr["fields"]["street_line_1"]).alias("street_line_1"),
                street_line_2_expr,
                street_line_3_expr,
                col(addr["fields"]["country"]).alias("country"),
                col(addr["fields"]["city"]).alias("city"),
                col(addr["fields"]["state_province"]).alias("state_province"),
                col(addr["fields"]["postal_code"]).alias("postal_code"),
                lit(True).alias("is_active"),
                lit("cc_salesforce").alias("created_by"),
                current_timestamp().alias("created_date"),
            ]
            print("after select_exprs ...")
            filtered_df = account_df.filter(address_condition).select(*select_exprs)
            address_dfs.append(filtered_df)
 
        transformed_physical_address = reduce(lambda df1, df2: df1.unionByName(df2), address_dfs)
        print("***************final_df****************")
        # transformed_physical_address.show()
        # print(transformed_physical_address.columns)
        transformed_physical_address.show()
        uuid_columns = ["related_identifier", "related_identifier_source", "physical_address_type", "street_line_1", "street_line_2", "country", "city", "state_province", "postal_code", "is_active"]
        transformed_physical_address = transformed_physical_address.withColumn(
            "physical_address_id",
            compute_uuid_udf(struct(*[col(c) for c in uuid_columns]))
        )
        # transformed_physical_address = transformed_physical_address.withColumn(
        #     "physical_address_id",
        #     compute_uuid_udf(struct(*transformed_physical_address.columns))
        # )
        print("hash_id creation is completed...")
        # print(transformed_physical_address.head(5))
 
        # Final static select/cast for output columns
        output_columns = [
            "CAST(lower(physical_address_id) AS STRING) AS physical_address_id",
            "CAST(lower(related_identifier) AS STRING) AS related_identifier",
            "CAST(lower(related_identifier_source) AS STRING) AS related_identifier_source",
            "CAST(lower(physical_address_type) AS STRING) AS physical_address_type",
            "CAST(lower(street_line_1) AS STRING) AS street_line_1",
            "CAST(lower(street_line_2) AS STRING) AS street_line_2",                   # Cast to STRING
            "CAST(street_line_3 AS STRING) AS street_line_3",                          # Cast to STRING
            "CAST(lower(country) AS STRING) AS country",
            "CAST(lower(city) AS STRING) AS city",
            "CAST(lower(state_province) AS STRING) AS state_province",
            "CAST(lower(postal_code) AS STRING) AS postal_code",
            "CAST(is_active AS BOOLEAN) AS is_active",
            "CAST(lower(created_by) AS STRING) AS created_by",
            "CAST(current_timestamp() AS TIMESTAMP) AS created_date"
        ]
 
        transformed_physical_address = transformed_physical_address.filter(
            col("physical_address_id").isNotNull() & col("related_identifier").isNotNull()
        ).selectExpr(*output_columns).distinct()
        print("after columns ordered...")
        print(transformed_physical_address.head(5))
 
        temp_path = "s3://app-id-111597-dep-id-114116-uu-id-9x0jt94siuto/sales_outputs/"
        transformed_physical_address.coalesce(1).write.mode("overwrite") \
            .option("header", "true") \
            .option("quote", '"') \
            .option("escape", '"') \
            .option("quoteAll", "true") \
            .option("multiLine", "true") \
            .option("timestampFormat", "yyyy-MM-dd HH:mm:ss") \
            .option("dateFormat", "yyyy-MM-dd") \
            .csv(temp_path)
        print("writing to s3 is completed...")

 
    except Exception as e:
        print('Error Message is:', e)
        return None
 
landing_tables = get_landing_data()
