from pyspark.sql import SparkSession
import psycopg2
from psycopg2 import extras
from urllib.parse import quote_plus
from datetime import date
import os
import csv


def sync_dataframe_with_postgresql(spark_df, db_config, table_name, compare_columns, schema_name, column_mapping, load_flag_column):
    """
    Syncs a PySpark DataFrame with a PostgreSQL table by comparing records and performing insert or update operations
    based on the load_flag column.

    :param spark_df: PySpark DataFrame containing the data to sync
    :param db_config: Dictionary with database connection parameters
    :param table_name: Name of the PostgreSQL table
    :param compare_columns: List of column names to compare for identifying existing records
    :param schema_name: Schema name of the PostgreSQL table
    :param column_mapping: Dictionary mapping DataFrame column names to PostgreSQL table column names
    :param load_flag_column: Column name indicating whether to insert or update
    """
    # Replace NaN values with None (to handle NULL in PostgreSQL)
    # In PySpark, we'll collect to driver for batch processing with psycopg2
    
    # Ensure column_mapping is initialized
    if column_mapping is None:
        column_mapping = {}
    
    # Map columns for database operations
    mapped_columns = {col: column_mapping.get(col, col) for col in spark_df.columns}
    
    # Convert PySpark DataFrame to list of dictionaries for processing
    # We'll use collect() to bring data to driver node - be cautious with large datasets
    rows = spark_df.toLocalIterator()
    
    # Connect to PostgreSQL
    conn = psycopg2.connect(
        host=db_config['host'],
        port=db_config['port'],
        dbname=db_config['database'],
        user=db_config['user'],
        password=db_config['password']
    )
    
    # Get the table's primary keys
    cursor = conn.cursor()
    cursor.execute(f"""
        SELECT a.attname
        FROM   pg_index i
        JOIN   pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
        WHERE  i.indrelid = '{schema_name}.{table_name}'::regclass
        AND    i.indisprimary;
    """)
    primary_keys = [row[0] for row in cursor.fetchall()]
    print(f"Primary keys for table {table_name}: {primary_keys}")
    
    # List to store mismatched records
    mismatched_records = []
    
    # Process rows in batches (better for large datasets)
    batch_size = 1000
    batch = []
    
    for row in rows:
        row_dict = row.asDict()
        
        # Check for empty strings and convert to None
        for key, value in row_dict.items():
            if isinstance(value, str) and (not value.strip() or value.strip().lower() == "nan"):
                row_dict[key] = None
        
        # Check the load_flag to determine whether to update or insert
        load_flag = row_dict.get(load_flag_column)
        
        try:
            if load_flag and load_flag.upper() == "U":
                # Build WHERE clause for comparison
                where_conditions = []
                where_values = []
                
                for col in compare_columns:
                    where_conditions.append(f"{mapped_columns[col]} = %s")
                    where_values.append(row_dict[col])
                
                where_clause = " AND ".join(where_conditions)
                
                # Check if record exists
                cursor.execute(
                    f"SELECT COUNT(*) FROM {schema_name}.{table_name} WHERE {where_clause}",
                    where_values
                )
                record_exists = cursor.fetchone()[0] > 0
                
                if record_exists:
                    # Record exists, perform update
                    update_columns = []
                    update_values = []
                    
                    for col in row_dict:
                        if (col not in compare_columns and 
                            mapped_columns[col] not in primary_keys and 
                            col != load_flag_column and 
                            col != "created_by"):
                            update_columns.append(f"{mapped_columns[col]} = %s")
                            update_values.append(row_dict[col])
                    
                    # Add audit fields
                    update_columns.append("last_updated_by = %s")
                    update_values.append(row_dict.get("created_by"))
                    
                    update_columns.append("last_updated_date = %s")
                    update_values.append(date.today())
                    
                    # Combine all values for the query
                    all_values = update_values + where_values
                    
                    # Execute update
                    update_query = f"""
                        UPDATE {schema_name}.{table_name}
                        SET {', '.join(update_columns)}
                        WHERE {where_clause}
                    """
                    cursor.execute(update_query, all_values)
                else:
                    # Record does not exist, add to mismatched records
                    mismatched_records.append(row_dict)
                    
            elif load_flag and load_flag.upper() == "I":
                # Build WHERE clause for comparison
                where_conditions = []
                where_values = []
                
                for col in compare_columns:
                    where_conditions.append(f"{mapped_columns[col]} = %s")
                    where_values.append(row_dict[col])
                
                where_clause = " AND ".join(where_conditions)
                
                # Check if record exists
                cursor.execute(
                    f"SELECT COUNT(*) FROM {schema_name}.{table_name} WHERE {where_clause}",
                    where_values
                )
                record_exists = cursor.fetchone()[0] > 0
                
                if record_exists:
                    # Record already exists, add to mismatched records
                    mismatched_records.append(row_dict)
                else:
                    # Prepare insert data
                    insert_columns = []
                    insert_values = []
                    placeholders = []
                    
                    for col in row_dict:
                        if col != load_flag_column:
                            insert_columns.append(mapped_columns[col])
                            insert_values.append(row_dict[col])
                            placeholders.append("%s")
                    
                    # Add is_active field
                    insert_columns.append("is_active")
                    insert_values.append(True)
                    placeholders.append("%s")
                    
                    # Add last_updated fields if not already in the data
                    if "last_updated_by" not in insert_columns:
                        insert_columns.append("last_updated_by")
                        insert_values.append(row_dict.get("created_by"))
                        placeholders.append("%s")
                    
                    if "last_updated_date" not in insert_columns:
                        insert_columns.append("last_updated_date")
                        insert_values.append(date.today())
                        placeholders.append("%s")
                    
                    # Execute insert
                    insert_query = f"""
                        INSERT INTO {schema_name}.{table_name} ({', '.join(insert_columns)})
                        VALUES ({', '.join(placeholders)})
                    """
                    cursor.execute(insert_query, insert_values)
            else:
                # Invalid load_flag, add to mismatched records
                mismatched_records.append(row_dict)
                
            # Add to batch and process if batch size reached
            batch.append(row_dict)
            if len(batch) >= batch_size:
                conn.commit()
                batch = []
                
        except Exception as e:
            # Catch any exceptions and add the record to mismatched records
            print(f"Error processing record: {row_dict}, Error: {e}")
            mismatched_records.append(row_dict)
    
    # Commit any remaining transactions
    if batch:
        conn.commit()
    
    # Close cursor and connection
    cursor.close()
    conn.close()
    
    # Write mismatched records to a CSV file
    if mismatched_records:
        mismatch_file = "mismatch_records.csv"
        
        if mismatched_records:
            # Get field names from the first record
            fieldnames = mismatched_records[0].keys()
            
            with open(mismatch_file, 'w', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(mismatched_records)
                
            print(f"Mismatched records written to {mismatch_file}")
            
            # Convert mismatched records back to a Spark DataFrame for further processing if needed
            mismatched_spark_df = spark.createDataFrame(mismatched_records)
            return mismatched_spark_df
    
    return None


def main():
    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("PostgreSQL Sync Job") \
        .getOrCreate()
    
    # Read data file into Spark DataFrame
    stage_file_path = "C:/Users/w619378/Downloads/b2b payables/upsert logic/card_revenues.csv"
    
    # Read CSV into Spark DataFrame
    spark_df = spark.read.option("header", "true").csv(stage_file_path)
    print("Input DataFrame schema:")
    spark_df.printSchema()
    print("Sample data:")
    spark_df.show(5)
    
    # Define the PostgreSQL connection parameters
    db_config = {
        'user': 'xbsddevdbAuroraAppAdmin',
        'password': 'rdsapg86a8-auroratdb-v1-node-1.chzdhpfung6p.us-east-1.rds.amazonaws.com:6160/?Action=connect&DBUser=xbsddevdbAuroraAppAdmin&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAXBZAIY4BSXTIKJ7D%2F20250513%2Fus-east-1%2Frds-db%2Faws4_request&X-Amz-Date=20250513T135421Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEEMaCXVzLWVhc3QtMSJGMEQCIExERx4B3gZL8dqpNsnytWASBHeZtFI9FgiwHPqxgcxwAiB6a%2FaV%2BXFWZG7zxlYYR9J1HvrEbk6tdPGoA4%2FPkHiUUirhAgjs%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDQ4NDg2MjExNzYzNSIMGx6SlkkYoQK7ktOUKrUCtwh57tTfWV23vIlkYIdWzBthKsfPMT%2BYggY9CiQLYc7WlB6ZJ%2BUNDYEDTy9uqE2%2BJppol0icnkc6ZyhmQOj7X1jDPJODr4VfDwWPl48LMpWCu15WgIACKORS2S6ovTMKPGEaTFV%2FPF6PLyIwcP%2B%2FoIxI1SbNwyhRZn9KotcyzXuWRUntUb9%2FS6eXnhSvG0Ww9J27%2FzfHbyuppwvEd3%2Buqp3KIMMvJSdQOFaPri5%2F00%2Fx%2FwhTYY86ZOVVhf2nSCp%2FOA0U2XbO%2FrELLZcM%2FMLLX0kYZxEHmBEan5ox7SETO8OXPoc6EwewuzV6fn066J22ks%2BzWjZ24%2BS%2B9yhkyXuBZYXRYhECYyD8G9HtbNVXhsiDd%2BG9RUSuz5Y14dOjHe8V6Khj75t2bduQiwRdG9wrAVoRgiOSMJ%2FGjMEGOqMBUOMjBgFh68ZWrzSeebEH6wLrLgFnQ5navqlAuqQBPpooqJigj2ANC1boPTyYi4e%2Fq5yQxP%2BOf%2FFg5LR6fwzu%2F5uR3lHKGXgNq%2BFejmpdimkS7ymfCxRPBW%2BSBSFiK7PnkSOgNkDpV6VDFjWFCMnFrWz6fgVSgEUmhOzGWOltx3pipJ7cPy44WvmJEx6%2F2zgLhuoj2Y%2BkWX%2FQW6kp9JIbok7n0w%3D%3D&X-Amz-Signature=f7373aa36ebfd18b7babfc571aa923fa3f6e698bc88f6ffd3012ff0cfee637ee',
        'host': 'nlb-rdsapg86a8-auroratdb-v1-fdcc221bbc9eea60.elb.us-east-1.amazonaws.com',
        'port': '6160',
        'database': 'xbsddevdb',
    }
    
    schema_name = "sds_main"
    table_name = "business_entity_card_revenues"
    compare_columns = ["stg_business_entity_id", "end_date"]
    column_mapping = {  # Map only the mismatched column
        "stg_business_entity_id": "business_entity_id"
    }
    load_flag_column = "load_flag"
    
    # Call the sync function
    mismatched_df = sync_dataframe_with_postgresql(
        spark_df, 
        db_config, 
        table_name, 
        compare_columns, 
        schema_name,
        column_mapping,
        load_flag_column
    )
    
    if mismatched_df:
        print("Mismatched records DataFrame:")
        mismatched_df.show(5)
    
    # Stop Spark session
    spark.stop()


if __name__ == "__main__":
    main()
