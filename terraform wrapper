def read_existing_records(schema_name, table_name, compare_columns, mapped_columns, df_values):
    """
    Reads existing records from PostgreSQL based on comparison columns.
    Uses IN clause with values from the DataFrame instead of a JOIN.
    """
    try:
        # Get JDBC connection parameters
        jdbc_url, user, password = get_jdbc_url()
        
        # Map the compare columns from DataFrame to DB columns
        db_columns = [mapped_columns.get(col, col) for col in compare_columns]
        logger.info(f"db_columns {db_columns}")
        
        # Collect distinct values for each comparison column
        value_lists = {}
        for col in compare_columns:
            # Convert Spark DataFrame column to a list of values
            values = [row[col] for row in df_values.select(col).distinct().collect()]
            value_lists[col] = values
            logger.info(f"Collected {len(values)} distinct values for column {col}")
        
        # Build WHERE conditions for each comparison column
        where_conditions = []
        for i, col in enumerate(compare_columns):
            db_col = mapped_columns.get(col, col)
            values = value_lists[col]
            
            # Handle NULL values separately
            non_null_values = [val for val in values if val is not None]
            has_nulls = len(non_null_values) < len(values)
            
            if non_null_values:
                # Format values appropriately based on their type
                formatted_values = []
                for val in non_null_values:
                    if isinstance(val, str):
                        # Escape single quotes properly
                        escaped_val = val.replace("'", "''")
                        formatted_values.append(f"'{escaped_val}'")
                    elif isinstance(val, bool):
                        formatted_values.append(str(val).lower())
                    elif val is None:
                        continue  # Skip nulls here, handled separately
                    else:
                        formatted_values.append(str(val))
                
                in_clause = f"{db_col} IN ({', '.join(formatted_values)})"
                if has_nulls:
                    where_conditions.append(f"({in_clause} OR {db_col} IS NULL)")
                else:
                    where_conditions.append(in_clause)
            elif has_nulls:
                where_conditions.append(f"{db_col} IS NULL")
        
        # Construct the full SQL query
        where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
        query = f"SELECT * FROM {schema_name}.{table_name} WHERE {where_clause}"
        
        logger.info(f"Executing query: {query}")
        
        # Execute the query
        existing_records = spark.read.format("jdbc").options(
            url=jdbc_url,
            query=query,
            user=user,
            password=password
        ).load()
        
        logger.info(f"Retrieved {existing_records.count()} existing records")
        return existing_records
    except Exception as e:
        logger.error(f"Error reading existing records: {e}")
        raise
