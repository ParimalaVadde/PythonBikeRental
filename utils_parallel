####################### utils.py ##########################################################################


import sys
import boto3
import json
import hashlib
import pandas as pd
import uuid
import psycopg2
from psycopg2 import sql
from concurrent.futures import ThreadPoolExecutor, as_completed
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import functions as F
import time
import builtins


def truncate_tables(rds_host, rds_port, db_name, db_user, rds_client, ssl_cert_s3_path, max_retries=3):
    """Truncate all tables in the sds_staging schema with token refresh support."""
    connection = None
    cursor = None
    retry_count = 0

    while retry_count < max_retries:
        try:
            # Generate a fresh token for this operation
            fresh_token = rds_client.generate_db_auth_token(
                DBHostname=rds_host,
                Port=rds_port,
                DBUsername=db_user
            )

            print(f"Connecting to database to truncate tables (attempt {retry_count + 1})...")

            connection = psycopg2.connect(
                host=rds_host,
                port=rds_port,
                database=db_name,
                user=db_user,
                password=fresh_token,
                sslmode="require",
                sslrootcert=ssl_cert_s3_path,
                connect_timeout=30
            )
            cursor = connection.cursor()

            # Fetch table names
            cursor.execute("SELECT table_name FROM information_schema.tables WHERE table_schema = 'sds_staging'")
            tables = [row[0] for row in cursor.fetchall()]

            if not tables:
                print("No tables found in sds_staging schema")
                return "No tables to truncate"

            print(f"Found {len(tables)} tables to truncate: {', '.join(tables)}")

            # Truncate all tables
            for table in tables:
                try:
                    truncate_query = sql.SQL("TRUNCATE TABLE {}.{} CASCADE").format(
                        sql.Identifier('sds_staging'),
                        sql.Identifier(table)
                    )
                    cursor.execute(truncate_query)
                    print(f"  Truncated table: sds_staging.{table}")
                except Exception as e:
                    print(f"  Error truncating {table}: {e}")

            connection.commit()
            print(f"✓ Successfully truncated {len(tables)} tables in sds_staging schema")
            return "Truncate success"

        except Exception as e:
            retry_count += 1
            msg = str(e)
            print(f"✗ Error truncating tables: {msg}")
            if connection:
                connection.rollback()

            if ("authentication failed" in msg.lower() or "pam authentication" in msg.lower()) and retry_count < max_retries:
                wait_time = retry_count * 2
                print(f"  Retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                if retry_count >= max_retries:
                    raise Exception(f"Failed to truncate tables after {max_retries} attempts: {msg}")
                else:
                    raise
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()


def write_single_table(table_name, df, glueContext, ss, rds_client, max_retries=3):
    """Write a single table to PostgreSQL using fresh IAM token on each retry."""
    retry_count = 0

    while retry_count < max_retries:
        try:
            # Always generate a new token inside the loop
            fresh_token = rds_client.generate_db_auth_token(
                DBHostname=ss.rds_host,
                Port=ss.rds_port,
                DBUsername=ss.db_user
            )

            print(f"[{table_name}] Attempt {retry_count + 1}: Generating fresh token and writing data...")

            row_count = df.count()
            optimal_partitions = builtins.max(4, builtins.min(20, row_count // 1000))
            df_optimized = df.repartition(optimal_partitions)

            # Use JDBC format to inject dynamic password
            jdbc_url = f"jdbc:postgresql://{ss.rds_host}:{ss.rds_port}/{ss.db_name}?sslmode=require&sslrootcert={ss.ssl_cert_s3_path}"

            df_optimized.write \
                .format("jdbc") \
                .option("url", jdbc_url) \
                .option("user", ss.db_user) \
                .option("password", fresh_token) \
                .option("dbtable", f"sds_staging.{table_name}") \
                .option("batchsize", 1000) \
                .option("isolationLevel", "READ_COMMITTED") \
                .mode("overwrite") \
                .save()

            print(f"[{table_name}] Data written successfully ({row_count} rows)")
            return (table_name, True, None)

        except Exception as e:
            retry_count += 1
            msg = str(e)
            print(f"[{table_name}] Error: {msg}")

            if ("authentication failed" in msg.lower() or "pam authentication" in msg.lower()) and retry_count < max_retries:
                wait_time = retry_count * 2
                print(f"[{table_name}] Retrying in {wait_time}s with new token...")
                time.sleep(wait_time)
                continue
            else:
                print(f"[{table_name}] Failed after {retry_count} attempts: {msg}")
                return (table_name, False, msg)

    return (table_name, False, f"Failed after {max_retries} retries")


def load_dataframes_to_postgres_parallel(dataframes_with_tables, glueContext, ss, rds_client,
                                         max_parallel_writes=4, max_retries=3):
    """Write multiple tables in parallel with guaranteed fresh token per write."""
    print(f"\n{'='*40}")
    print(f"Starting parallel load for {len(dataframes_with_tables)} tables")
    print(f"{'='*40}\n")

    results = {"successful": [], "failed": [], "start_time": time.time()}

    with ThreadPoolExecutor(max_workers=max_parallel_writes) as executor:
        futures = {
            executor.submit(write_single_table, t, df, glueContext, ss, rds_client, max_retries): t
            for t, df in dataframes_with_tables.items()
        }

        for future in as_completed(futures):
            table_name = futures[future]
            try:
                t, success, err = future.result()
                if success:
                    results["successful"].append(t)
                else:
                    results["failed"].append({"table": t, "error": err})
            except Exception as e:
                results["failed"].append({"table": table_name, "error": str(e)})

    duration = round(time.time() - results["start_time"], 2)
    print(f"\n Load Summary:")
    print(f"  Successful: {len(results['successful'])}")
    print(f"  Failed: {len(results['failed'])}")
    print(f"  Duration: {duration}s\n")

    if results["failed"]:
        failed = [f['table'] for f in results['failed']]
        raise Exception(f"Failed tables: {', '.join(failed)}")

    results["duration_seconds"] = duration
    return results
