import sys
import boto3
import json
import hashlib
import pandas as pd
import uuid
import psycopg2
from psycopg2 import sql
from concurrent.futures import ThreadPoolExecutor, as_completed
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import functions as F
import time


def truncate_tables(rds_host, rds_port, db_name, db_user, rds_client, ssl_cert_s3_path, max_retries=3):
    """Truncate all tables in the sds_staging schema with token refresh support."""
    connection = None
    cursor = None
    retry_count = 0

    while retry_count < max_retries:
        try:
            # Generate a fresh token for this operation
            fresh_token = rds_client.generate_db_auth_token(
                DBHostname=rds_host,
                Port=rds_port,
                DBUsername=db_user
            )

            print(f"Connecting to database to truncate tables (attempt {retry_count + 1})...")

            connection = psycopg2.connect(
                host=rds_host,
                port=rds_port,
                database=db_name,
                user=db_user,
                password=fresh_token,
                sslmode="require",
                sslrootcert=ssl_cert_s3_path,
                connect_timeout=30
            )
            cursor = connection.cursor()

            # Fetch table names
            cursor.execute("SELECT table_name FROM information_schema.tables WHERE table_schema = 'sds_staging'")
            tables = [row[0] for row in cursor.fetchall()]

            if not tables:
                print("No tables found in sds_staging schema")
                return "No tables to truncate"

            print(f"Found {len(tables)} tables to truncate")

            # Truncate all tables
            truncated_count = 0
            for table in tables:
                try:
                    truncate_query = sql.SQL("TRUNCATE TABLE {}.{} CASCADE").format(
                        sql.Identifier('sds_staging'),
                        sql.Identifier(table)
                    )
                    cursor.execute(truncate_query)
                    truncated_count += 1
                    print(f"  ✓ Truncated: {table}")
                except Exception as e:
                    print(f"  ✗ Error truncating {table}: {e}")

            connection.commit()
            print(f"✓ Successfully truncated {truncated_count}/{len(tables)} tables")
            return "Truncate success"

        except Exception as e:
            retry_count += 1
            msg = str(e)
            print(f"✗ Error truncating tables: {msg}")
            if connection:
                connection.rollback()

            if ("authentication failed" in msg.lower() or "pam authentication" in msg.lower()) and retry_count < max_retries:
                wait_time = retry_count * 2
                print(f"  Retrying in {wait_time}s with fresh token...")
                time.sleep(wait_time)
            else:
                if retry_count >= max_retries:
                    raise Exception(f"Failed to truncate tables after {max_retries} attempts: {msg}")
                else:
                    raise
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()


def write_single_table(table_name, df, glueContext, ss, rds_client, max_retries=3):
    """
    Write a single table to PostgreSQL using fresh IAM token on each retry.
    
    IMPORTANT: DataFrame must be cached/materialized before calling this function
    to avoid re-executing transformations with expired tokens.
    """
    retry_count = 0

    while retry_count < max_retries:
        try:
            # Generate fresh token for this write attempt
            fresh_token = rds_client.generate_db_auth_token(
                DBHostname=ss.rds_host,
                Port=ss.rds_port,
                DBUsername=ss.db_user
            )

            print(f"[{table_name}] Attempt {retry_count + 1}: Writing with fresh token...")

            # Don't call df.count() here - it may re-execute the entire transformation chain
            # with the old expired token. Instead, just write directly.
            
            # Optimize partitioning (default to 10 if we can't get count easily)
            # Only try to get partition count if df is cached
            try:
                # Try to get a rough partition count without triggering full execution
                num_partitions = df.rdd.getNumPartitions()
                optimal_partitions = max(4, min(20, num_partitions))
                df_optimized = df.repartition(optimal_partitions)
            except:
                # If we can't get partitions safely, just use the df as-is
                df_optimized = df
                print(f"[{table_name}] Using DataFrame as-is (partitioning not optimized)")

            # Use JDBC write with fresh token
            jdbc_url = f"jdbc:postgresql://{ss.rds_host}:{ss.rds_port}/{ss.db_name}?sslmode=require&sslrootcert={ss.ssl_cert_s3_path}"

            df_optimized.write \
                .format("jdbc") \
                .option("url", jdbc_url) \
                .option("user", ss.db_user) \
                .option("password", fresh_token) \
                .option("dbtable", f"sds_staging.{table_name}") \
                .option("batchsize", 1000) \
                .option("isolationLevel", "READ_COMMITTED") \
                .mode("append") \
                .save()

            print(f"✓ [{table_name}] Data written successfully")
            return (table_name, True, None)

        except Exception as e:
            retry_count += 1
            msg = str(e)
            
            # Check if it's an auth error
            if "authentication failed" in msg.lower() or "pam authentication" in msg.lower():
                if retry_count < max_retries:
                    wait_time = retry_count * 2
                    print(f"✗ [{table_name}] Auth error. Retrying in {wait_time}s... (Attempt {retry_count + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"✗ [{table_name}] Failed after {max_retries} attempts: {msg[:200]}")
                    return (table_name, False, msg)
            else:
                # Non-auth error, don't retry
                print(f"✗ [{table_name}] Non-auth error: {msg[:200]}")
                return (table_name, False, msg)

    return (table_name, False, f"Failed after {max_retries} retries")


def load_dataframes_to_postgres_parallel(dataframes_with_tables, glueContext, ss, rds_client,
                                         max_parallel_writes=4, max_retries=3):
    """
    Write multiple tables in parallel with guaranteed fresh token per write.
    
    IMPORTANT: All DataFrames should be cached/materialized before calling this function.
    """
    print(f"\n{'='*80}")
    print(f"PARALLEL DATA LOADING")
    print(f"{'='*80}")
    print(f"Tables to load: {len(dataframes_with_tables)}")
    print(f"Max parallel writes: {max_parallel_writes}")
    print(f"Max retries per table: {max_retries}")
    print(f"{'='*80}\n")

    results = {
        "successful": [],
        "failed": [],
        "start_time": time.time()
    }

    # Write tables in parallel
    with ThreadPoolExecutor(max_workers=max_parallel_writes) as executor:
        futures = {
            executor.submit(
                write_single_table,
                table_name,
                df,
                glueContext,
                ss,
                rds_client,
                max_retries
            ): table_name
            for table_name, df in dataframes_with_tables.items()
        }

        # Process results as they complete
        for future in as_completed(futures):
            table_name = futures[future]
            try:
                t, success, err = future.result()
                if success:
                    results["successful"].append(t)
                else:
                    results["failed"].append({"table": t, "error": err})
            except Exception as e:
                print(f"✗ Unexpected error for {table_name}: {e}")
                results["failed"].append({"table": table_name, "error": str(e)})

    # Calculate duration
    duration = round(time.time() - results["start_time"], 2)
    
    # Print summary
    print(f"\n{'='*80}")
    print(f"LOAD SUMMARY")
    print(f"{'='*80}")
    print(f"Total tables: {len(dataframes_with_tables)}")
    print(f"Successful: {len(results['successful'])}")
    print(f"Failed: {len(results['failed'])}")
    print(f"Duration: {duration} seconds")
    
    if results["successful"]:
        print(f"\n✓ Successfully loaded tables:")
        for table in results["successful"]:
            print(f"  - {table}")
    
    if results["failed"]:
        print(f"\n✗ Failed tables:")
        for failure in results["failed"]:
            print(f"  - {failure['table']}")
            print(f"    Error: {failure['error'][:150]}...")
    
    print(f"{'='*80}\n")

    # Raise exception if any tables failed
    if results["failed"]:
        failed_tables = [f['table'] for f in results['failed']]
        raise Exception(f"Failed to load {len(results['failed'])} tables: {', '.join(failed_tables)}")

    results["duration_seconds"] = duration
    return results
