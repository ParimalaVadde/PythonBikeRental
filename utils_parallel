import sys
import boto3
import json
import hashlib
import pandas as pd
import uuid
import psycopg2
from psycopg2 import sql
from concurrent.futures import ThreadPoolExecutor, as_completed
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col, explode, lit, when, udf, to_date, current_date, coalesce
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, IntegerType, MapType
from pyspark.sql.utils import AnalysisException
from datetime import datetime
from pyspark.sql.functions import when, lit, col, explode, from_json
from pyspark.sql.functions import concat_ws, current_timestamp
from pyspark.sql import DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
import staging_schema as ss
import time


def truncate_tables(rds_host, rds_port, db_name, db_user, rds_client, ssl_cert_s3_path, max_retries=3):
    """
    Truncate all tables in the sds_staging schema with token refresh support.
    
    Args:
        rds_host: Database host
        rds_port: Database port
        db_name: Database name
        db_user: Database user
        rds_client: Boto3 RDS client for token generation
        ssl_cert_s3_path: SSL certificate path
        max_retries: Maximum retry attempts
    
    Returns:
        str: Success message
    """
    connection = None
    cursor = None
    retry_count = 0

    while retry_count < max_retries:
        try:
            # Generate a fresh token for this operation
            fresh_token = rds_client.generate_db_auth_token(
                DBHostname=rds_host,
                Port=rds_port,
                DBUsername=db_user
            )
            
            print(f"Connecting to database to truncate tables (attempt {retry_count + 1})...")
            
            # Establish a connection to the database
            connection = psycopg2.connect(
                host=rds_host,
                port=rds_port,
                database=db_name,
                user=db_user,
                password=fresh_token,
                sslmode="require",
                sslrootcert=ssl_cert_s3_path,
                connect_timeout=30  # Add timeout
            )
            cursor = connection.cursor()

            # Fetch all table names in the schema
            selecting_tables = "SELECT table_name FROM information_schema.tables WHERE table_schema = 'sds_staging' ORDER BY table_name"
            cursor.execute(selecting_tables)
            tables = [row[0] for row in cursor.fetchall()]
            
            if not tables:
                print("No tables found in sds_staging schema")
                return "No tables to truncate"
            
            print(f"Found {len(tables)} tables to truncate: {', '.join(tables)}")

            # Truncate each table
            truncated_count = 0
            for table in tables:
                try:
                    # Use parameterized query for safety
                    truncate_query = sql.SQL("TRUNCATE TABLE {}.{} CASCADE").format(
                        sql.Identifier('sds_staging'),
                        sql.Identifier(table)
                    )
                    cursor.execute(truncate_query)
                    truncated_count += 1
                    print(f" Truncated table: sds_staging.{table}")
                except Exception as table_error:
                    print(f" Error truncating sds_staging.{table}: {table_error}")
                    # Continue with other tables

            # Commit the transaction
            connection.commit()
            print(f"\n✓ Successfully truncated {truncated_count}/{len(tables)} tables in sds_staging schema")
            return f"Truncated {truncated_count} tables successfully"

        except Exception as e:
            error_msg = str(e)
            retry_count += 1
            
            print(f"✗ Error truncating tables: {error_msg}")
            
            if connection:
                connection.rollback()
            
            # Check if it's an authentication error
            if ("authentication failed" in error_msg.lower() or 
                "pam authentication" in error_msg.lower()) and retry_count < max_retries:
                wait_time = retry_count * 2
                print(f"  Retrying in {wait_time} seconds... (Attempt {retry_count}/{max_retries})")
                time.sleep(wait_time)
            else:
                if retry_count >= max_retries:
                    raise Exception(f"Failed to truncate tables after {max_retries} attempts: {error_msg}")
                else:
                    raise

        finally:
            # Close the cursor and connection if they were created
            if cursor:
                cursor.close()
            if connection:
                connection.close()
    
    raise Exception(f"Failed to truncate tables after {max_retries} attempts")


def write_single_table(table_name, df, glueContext, ss, rds_client, max_retries=3):
    """
    Write a single table to PostgreSQL with fresh token generation and retry logic.
    
    Args:
        table_name (str): Target table name
        df (DataFrame): PySpark DataFrame to write
        glueContext: AWS Glue context
        ss: Schema/settings object with DB connection details
        rds_client: Boto3 RDS client for token generation
        max_retries (int): Maximum number of retry attempts
    
    Returns:
        tuple: (table_name, success_status, error_message)
    """
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # Generate a fresh token for this specific write operation
            fresh_token = rds_client.generate_db_auth_token(
                DBHostname=ss.rds_host,
                Port=ss.rds_port,
                DBUsername=ss.db_user
            )
            
            print(f"[{table_name}] Attempt {retry_count + 1}: Generating fresh token and writing data...")
            
            # Optimize DataFrame before writing
            row_count = df.count()
            optimal_partitions = max(4, min(20, row_count // 1000))
            df_optimized = df.repartition(optimal_partitions)
            
            # Convert to DynamicFrame
            dynamic_frame = DynamicFrame.fromDF(df_optimized, glueContext, table_name)
            
            # Connection options with fresh token
            connection_options = {
                "url": f"jdbc:postgresql://{ss.rds_host}:{ss.rds_port}/{ss.db_name}?sslmode=require&sslrootcert={ss.ssl_cert_s3_path}",
                "user": ss.db_user,
                "password": fresh_token,
                "dbtable": f"sds_staging.{table_name}",
                "batchsize": "1000",
                "isolationLevel": "READ_COMMITTED"
            }
            
            # Write the DynamicFrame
            glueContext.write_dynamic_frame.from_options(
                frame=dynamic_frame,
                connection_type="postgresql",
                connection_options=connection_options,
                transformation_ctx=f"write_{table_name}"
            )
            
            print(f"[{table_name}] Data written successfully ({row_count} rows)")
            return (table_name, True, None)
            
        except Exception as e:
            retry_count += 1
            error_msg = str(e)
            
            if "PAM authentication failed" in error_msg or "authentication failed" in error_msg:
                if retry_count < max_retries:
                    wait_time = retry_count * 2
                    print(f"[{table_name}] Authentication failed. Retrying in {wait_time}s... (Attempt {retry_count}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    print(f"[{table_name}] Failed after {max_retries} attempts: {error_msg}")
                    return (table_name, False, error_msg)
            else:
                print(f"[{table_name}] Error: {error_msg}")
                return (table_name, False, error_msg)
    
    return (table_name, False, f"Failed after {max_retries} retries")


def load_dataframes_to_postgres_parallel(dataframes_with_tables, glueContext, ss, rds_client, 
                                         max_parallel_writes=4, max_retries=3):
    """
    Load multiple DataFrames to PostgreSQL in parallel with fresh token generation per write.
    
    Args:
        dataframes_with_tables (dict): Dictionary of {table_name: dataframe}
        glueContext: AWS Glue context
        ss: Schema/settings object
        rds_client: Boto3 RDS client for token generation
        max_parallel_writes (int): Number of tables to write in parallel (default: 4)
        max_retries (int): Maximum retry attempts per table (default: 3)
    
    Returns:
        dict: Summary of write operations
    """
    print(f"\n{'='*40}")
    print(f"Starting parallel data loading for {len(dataframes_with_tables)} tables")
    print(f"Max parallel writes: {max_parallel_writes}")
    print(f"Max retries per table: {max_retries}")
    print(f"{'='*40}\n")
    
    results = {
        'successful': [],
        'failed': [],
        'total_tables': len(dataframes_with_tables),
        'start_time': time.time()
    }
    
    # Use ThreadPoolExecutor for parallel writes
    with ThreadPoolExecutor(max_workers=max_parallel_writes) as executor:
        # Submit all write jobs
        future_to_table = {
            executor.submit(
                write_single_table, 
                table_name, 
                df, 
                glueContext, 
                ss, 
                rds_client,
                max_retries
            ): table_name 
            for table_name, df in dataframes_with_tables.items()
        }
        
        # Process completed jobs as they finish
        for future in as_completed(future_to_table):
            table_name = future_to_table[future]
            try:
                table_name, success, error_msg = future.result()
                
                if success:
                    results['successful'].append(table_name)
                else:
                    results['failed'].append({
                        'table': table_name,
                        'error': error_msg
                    })
            except Exception as e:
                print(f"Unexpected error processing {table_name}: {e}")
                results['failed'].append({
                    'table': table_name,
                    'error': str(e)
                })
    
    # Calculate duration
    results['duration_seconds'] = round(time.time() - results['start_time'], 2)
    
    # Print summary
    print(f"\n{'='*40}")
    print(f"LOAD SUMMARY")
    print(f"{'='*40}")
    print(f"Total tables: {results['total_tables']}")
    print(f"Successful: {len(results['successful'])}")
    print(f"Failed: {len(results['failed'])}")
    print(f"Duration: {results['duration_seconds']} seconds")
    
    if results['successful']:
        print(f"\n✓ Successfully loaded tables:")
        for table in results['successful']:
            print(f"  - {table}")
    
    if results['failed']:
        print(f"\n✗ Failed tables:")
        for failure in results['failed']:
            print(f"  - {failure['table']}: {failure['error'][:100]}...")
    
    print(f"{'='*40}\n")
    
    # Raise exception if any tables failed
    if results['failed']:
        failed_tables = [f['table'] for f in results['failed']]
        raise Exception(f"Failed to load {len(results['failed'])} tables: {', '.join(failed_tables)}")
    
    return results
