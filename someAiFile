decimal_columns = ["reported_annual_revenue","sum_of_payments","chargeback_amount","chargeback_percentage","payment_terms_discount_rate","card_revenue__1m__average_monthly_amount","card_revenue__3m__average_monthly_amount","card_revenue__12m__average_monthly_amount","card_transactions_stability__1m__daily_coverage_ratio","card_transactions_stability__1m__weekly_coverage_ratio","card_transactions_stability__1m__monthly_coverage_ratio","card_transactions_stability__3m__daily_coverage_ratio","card_transactions_stability__3m__weekly_coverage_ratio","card_transactions_stability__3m__monthly_coverage_ratio","card_transactions_stability__12m__daily_coverage_ratio","card_transactions_stability__12m__weekly_coverage_ratio","card_transactions_stability__12m__monthly_coverage_ratio"]


this is the pyspark df transformed_revenue

if any columns of transformed_revenue present in decimal_columns

then i need to round off decimal value to 2 points

like 
from 8815233.7777  to   8815233.78


make changes in below code

# Transform `transformed_revenue`
    transformed_revenue = transformed_json_df.select(*ss.revenue).withColumnRenamed(
        "card_revenue__end_date", "end_date"
    )

    transformed_revenue = transformed_revenue.withColumn(
    "is_active", 
    lit(True)  # This creates a Boolean column with True values for all rows
    )
    
    print("********************business_entity_card_revenues***************************************")
    print(sorted(transformed_revenue.columns))
    print("**********************************************************************************")
    transformed_revenue = transformed_revenue.withColumn(
        "revenue_id",
        compute_uuid_udf(struct(*transformed_revenue.columns))
    ).dropDuplicates()
	
	
	
the changes shoukd happen after 
	transformed_revenue = transformed_revenue.withColumn(
    "is_active", 
    lit(True)  # This creates a Boolean column with True values for all rows
    )
