decimal_columns = ["reported_annual_revenue","sum_of_payments","chargeback_amount","chargeback_percentage","payment_terms_discount_rate","card_revenue__1m__average_monthly_amount","card_revenue__3m__average_monthly_amount","card_revenue__12m__average_monthly_amount","card_transactions_stability__1m__daily_coverage_ratio","card_transactions_stability__1m__weekly_coverage_ratio","card_transactions_stability__1m__monthly_coverage_ratio","card_transactions_stability__3m__daily_coverage_ratio","card_transactions_stability__3m__weekly_coverage_ratio","card_transactions_stability__3m__monthly_coverage_ratio","card_transactions_stability__12m__daily_coverage_ratio","card_transactions_stability__12m__weekly_coverage_ratio","card_transactions_stability__12m__monthly_coverage_ratio"]


this is the pyspark df transformed_revenue

if any columns of transformed_revenue present in decimal_columns

then i need to round off decimal value to 2 points

like 
from 8815233.7777  to   8815233.78


make changes in below code

# Transform `transformed_revenue`
    transformed_revenue = transformed_json_df.select(*ss.revenue).withColumnRenamed(
        "card_revenue__end_date", "end_date"
    )

    transformed_revenue = transformed_revenue.withColumn(
    "is_active", 
    lit(True)  # This creates a Boolean column with True values for all rows
    )
    
    print("********************business_entity_card_revenues***************************************")
    print(sorted(transformed_revenue.columns))
    print("**********************************************************************************")
    transformed_revenue = transformed_revenue.withColumn(
        "revenue_id",
        compute_uuid_udf(struct(*transformed_revenue.columns))
    ).dropDuplicates()
	
	
	
the changes shoukd happen after 
	transformed_revenue = transformed_revenue.withColumn(
    "is_active", 
    lit(True)  # This creates a Boolean column with True values for all rows
    )



Here's the modified code with the decimal rounding logic added after the `is_active` column creation:

```python
# Transform `transformed_revenue`
transformed_revenue = transformed_json_df.select(*ss.revenue).withColumnRenamed(
    "card_revenue__end_date", "end_date"
)

transformed_revenue = transformed_revenue.withColumn(
    "is_active", 
    lit(True)  # This creates a Boolean column with True values for all rows
)

# Round decimal columns to 2 decimal places
decimal_columns = ["reported_annual_revenue","sum_of_payments","chargeback_amount","chargeback_percentage","payment_terms_discount_rate","card_revenue__1m__average_monthly_amount","card_revenue__3m__average_monthly_amount","card_revenue__12m__average_monthly_amount","card_transactions_stability__1m__daily_coverage_ratio","card_transactions_stability__1m__weekly_coverage_ratio","card_transactions_stability__1m__monthly_coverage_ratio","card_transactions_stability__3m__daily_coverage_ratio","card_transactions_stability__3m__weekly_coverage_ratio","card_transactions_stability__3m__monthly_coverage_ratio","card_transactions_stability__12m__daily_coverage_ratio","card_transactions_stability__12m__weekly_coverage_ratio","card_transactions_stability__12m__monthly_coverage_ratio"]

# Get columns present in both transformed_revenue and decimal_columns
columns_to_round = [col for col in decimal_columns if col in transformed_revenue.columns]

# Apply rounding to decimal columns
for col_name in columns_to_round:
    transformed_revenue = transformed_revenue.withColumn(
        col_name,
        round(col(col_name), 2)
    )

print("********************business_entity_card_revenues***************************************")
print(sorted(transformed_revenue.columns))
print("**********************************************************************************")
transformed_revenue = transformed_revenue.withColumn(
    "revenue_id",
    compute_uuid_udf(struct(*transformed_revenue.columns))
).dropDuplicates()
```

The key changes:
1. Added the `decimal_columns` list after the `is_active` column creation
2. Created `columns_to_round` to find intersection of columns present in both the dataframe and the decimal_columns list
3. Used a loop to apply `round(col(col_name), 2)` to each decimal column that exists in the dataframe
4. This will round values like `8815233.7777` to `8815233.78` as requested

Make sure you have the necessary imports at the top of your file:
```python
from pyspark.sql.functions import round, col
```
