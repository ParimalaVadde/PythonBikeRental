# Call the `relationship` function and assign the result to `transformed_relationship`
transformed_relationship = relationship(transformed_business_entity, transformed_contacts, transformed_df)

transformed_relationship = transformed_relationship.withColumn(
    "is_active", 
    lit(True)  # This creates a Boolean column with True values for all rows
)

print("********************relationship***************************************")
print(sorted(transformed_relationship.columns))
print("**********************************************************************************")

transformed_relationship_columns = [
    col for col in transformed_relationship.columns 
    if col not in ['client_ecid', 'client_vendor_id', 'client_vendor_site_id']
]
transformed_relationship = transformed_relationship.withColumn(
    "business_entity_relationship_id",
    compute_uuid_udf(struct(*[col(c) for c in transformed_relationship_columns]))
)
# transformed_relationship = transformed_relationship.withColumn(
#     "business_entity_relationship_id",
#     compute_uuid_udf(struct(*transformed_relationship.columns))
# )


# Show distinct rows in the resulting DataFrame
transformed_relationship.distinct().show(truncate=False)



def write_single_csv_to_s3_fixed(df: DataFrame, s3_bucket: str, s3_key: str, temp_path: str):
    """
    Writes a Spark DataFrame as a single CSV file with proper escaping to S3.
    """
    # Coalesce to one partition and write to temp S3 directory with proper CSV options
    df.coalesce(1).write.mode("overwrite") \
        .option("header", True) \
        .option("quote", '"') \
        .option("escape", '"') \
        .option("quoteAll", True) \
        .option("multiLine", True) \
        .csv(temp_path)

    # Rest of your S3 copy logic remains the same
    s3 = boto3.client("s3")
    temp_bucket = temp_path.replace("s3://", "").split("/")[0]
    temp_prefix = "/".join(temp_path.replace("s3://", "").split("/")[1:])

    objects = s3.list_objects_v2(Bucket=temp_bucket, Prefix=temp_prefix)
    part_file = next(
        obj['Key'] for obj in objects.get('Contents', []) if obj['Key'].endswith(".csv")
    )

    s3.copy_object(
        Bucket=s3_bucket,
        CopySource={'Bucket': temp_bucket, 'Key': part_file},
        Key=s3_key
    )

    # Clean up temp files
    for obj in objects.get('Contents', []):
        s3.delete_object(Bucket=temp_bucket, Key=obj['Key'])

write_single_csv_to_s3_fixed(
        transformed_relationship,
        s3_bucket="app-id-111597-dep-id-114116-uu-id-9x0jt94siuto",
        s3_key="upsert/pfd_staging/transformed_relationship.csv",
        temp_path="s3://app-id-111597-dep-id-114116-uu-id-9x0jt94siuto/upsert/pfd_staging/_tmp"
    )
	
	
i m getting below error
Error Category: QUERY_ERROR; Failed Line Number: 1071; AnalysisException: CSV data source does not support void data type.
