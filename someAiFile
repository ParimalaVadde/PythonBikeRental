Title: Landing to Staging Process - KT for B2B Supplier Directory Services

Slide 1: Introduction - My Scope in the Project

Speaking Notes:
Hi, I'm currently working on the ETL staging layer of the B2B Supplier Directory Services project for JPMC. Our project focuses on creating a centralized system to manage supplier data. I will be walking you through the staging process, i.e., how data is transformed from the landing schema to the staging schema.

Slide 2: Project Background

Slide Text:

Client: JPMorgan Chase (JPMC)

Vendor: Mphasis

Domain: Banking / B2B Supplier Management

Goal: Centralized supplier repository, enhanced supplier profiling, and spend/payment insights

Slide 3: Where I Fit - Landing to Staging

Slide Text:

Source: CSV files arrive in S3 bucket from external sources like Diagnostic

Landing: ECS job loads raw CSV into PostgreSQL landing schema

Staging: Glue job processes landing data into transformed structured tables

Speaking Notes:
My focus is on Step 3. From landing, we apply transformations using PySpark in AWS Glue and load data into normalized staging tables in PostgreSQL.

Slide 4: Source Table - landing.analysis_pfd

Slide Text:

Flat columns + Nested JSON columns

Examples:

vendor_id, site_id, client_ecid

associated_people, card_revenues, registered_agents

Speaking Notes:
The landing table holds raw data, including nested structures that must be flattened. This becomes our input.

Slide 5: Staging Tables Created

Slide Text:
We create ~12 structured tables:

business_entity_contacts

business_entity_card_revenues

business_entity_card_transactions_stability

telecommunication_address

electronic_address

business_entity_receivables_attribute

industry_classification, characteristics

physical_address, restrictions, relationships, identifiers

Speaking Notes:
Each table represents a specific aspect of supplier information.

Slide 6: Table Example - business_entity_contacts

Slide Text:
Input: associated_people JSON

Transformation:

Flatten nested structure using flatten_nested_json_column

Explode titles[]

Output Columns:

stg_business_entity_id

contact_name

contact_title

business_entity_contact_id (UUID)

Slide 7: Table Example - business_entity_card_revenues

Input: card_revenues JSON
Output:

stg_business_entity_id

end_date, 1m.average_monthly_amount, etc.

revenue_id (UUID)

Notes: Flattened & exploded, then deduplicated

Slide 8: Table Example - business_entity_receivables_attribute

Input: Multiple flat columns like:

average_vendor_dso, vc_concierge, has_online_payments

Transformation:

Use melt_dataframe()

Output:

stg_business_entity_id

receivables_attribute_type

receivables_attribute_value

Slide 9: Table Example - relationships and identifiers

Relationships:

Derived using filter_interco, client_ecid logic

Joins with contacts to link entity roles

Identifiers:

Union of:

Business-level identifiers (from transformed_df)

Relationship-level identifiers (from relationship table)

Output:
| identifier_type | identifier_value | related_identifier | source |

Slide 10: Summary

Slide Text:

Complex raw data from landing transformed to 12+ staging tables

Uses PySpark + Glue + Postgres

Utilities: flatten_nested_json_column(), melt_dataframe()

UUIDs assigned to each entity for integrity

Speaking Notes:
The staging job bridges raw CSV and structured insights, normalizing and mapping all fields needed by the main layer.
