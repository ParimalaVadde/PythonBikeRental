################################ pfd_staging ##################################################


import sys
import boto3
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext
import staging_schema as ss
import utils as utl
import time

# Get job args
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

# Spark/Glue context setup
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Initialize RDS client
rds_client = boto3.client("rds", region_name=ss.region)

# Step 1: Extract landing data
print("\n" + "="*80)
print("STEP 1: EXTRACTING LANDING DATA")
print("="*80)

source_df = glueContext.read.format("jdbc").option(
    "url", f"jdbc:postgresql://{ss.rds_host}:{ss.rds_port}/{ss.db_name}?sslmode=require&sslrootcert={ss.ssl_cert_s3_path}"
).option(
    "user", ss.db_user
).option(
    "password", rds_client.generate_db_auth_token(
        DBHostname=ss.rds_host, Port=ss.rds_port, DBUsername=ss.db_user
    )
).option(
    "query", "SELECT * FROM sds_landing.analysis_pfd WHERE filter_employee='N'"
).load()

print(f"✅ Fetched {source_df.count()} rows from landing data")

# Step 2: Transformations (placeholder)
print("\n" + "="*80)
print("STEP 2: TRANSFORMING DATA")
print("="*80)
# (Your transformation logic remains unchanged)
# transformed_business_entity = ...
# etc.

# Step 3: Prepare table map
print("\n" + "="*80)
print("STEP 3: PREPARING DATA FOR LOAD")
print("="*80)
dataframes_with_tables = {
    "business_entity": transformed_business_entity,
    "business_entity_details": transformed_business_entity_details,
    # ... other tables ...
    "business_entity_relationships": transformed_relationship
}
print(f"Prepared {len(dataframes_with_tables)} tables for loading.")

# Step 4: Truncate existing staging tables
print("\n" + "="*80)
print("STEP 4: TRUNCATING STAGING TABLES")
print("="*80)
utl.truncate_tables(ss.rds_host, ss.rds_port, ss.db_name, ss.db_user, rds_client, ss.ssl_cert_s3_path)

# Step 5: Parallel load
print("\n" + "="*80)
print("STEP 5: LOADING DATA TO POSTGRES (PARALLEL)")
print("="*80)
load_results = utl.load_dataframes_to_postgres_parallel(
    dataframes_with_tables=dataframes_with_tables,
    glueContext=glueContext,
    ss=ss,
    rds_client=rds_client,
    max_parallel_writes=4,
    max_retries=3
)

print(f"\n✅ All data loaded successfully in {load_results['duration_seconds']} seconds")
job.commit()
print("\n" + "="*80)
print("JOB COMPLETED SUCCESSFULLY")
print("="*80)
