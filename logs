B2B Supplier Directory Services – AWS Glue ETL (Landing to Staging)


Flow Diagram (Landing → Staging)

Landing (PostgreSQL)
↓
Extract via GlueContext (JDBC)
↓
Flatten Nested JSONs
↓
Transform + UUID Generation
↓
Business Entity Creation
↓
Relationship Mapping
↓
Identifier Normalization
↓
Schema Alignment
↓
Truncate Staging Tables
↓
Load to PostgreSQL Staging
↓
Job Commit


High-Level Workflow
Landing → PySpark Transformations → Flattening → Relationship Mapping → UUID Generation → Schema Alignment → Load to Staging

High-Level Steps

1) Establish JDBC connection to AWS RDS PostgreSQL database

* Generate IAM authentication token using 'boto3.rds.generate_db_auth_token()'
* Build JDBC connection properties (URL, user, password with SSL mode enabled).

2) Initialize AWS Glue & Spark contexts

* Create 'SparkContext', 'GlueContext', and 'SparkSession' for ETL execution.
* Initialize the Glue job with 'Job(glueContext)'.

3) Fetch input data from landing schema ('sds_landing.analysis_pfd')

* Read data via JDBC with filter condition 'filter_employee = 'N''.
* Load it into a Spark DataFrame 'source_df'.

4) Add business entity UUIDs for key columns

* Compute and append uuid

5) Validate and sanitize mandatory columns

* Compare input DataFrame columns with required list from 'staging_schema.py' ('pfd_mandatory_columns').
* Add missing columns with 'null' values if not present in the input.

6) Read PFD mapping file and rename columns as per staging standards

* Load mapping config ('pfd_mapping') from S3 using 'utils.read_map_file()'.
* Create mapping dictionary '{source_column → target_column}'.
* Rename DataFrame columns accordingly.

7) Add JPMC business entity ID conditionally

* Populate 'stg_jpmc_business_entity_id' if 'client_ecid' is not null or  'supplier_ecid' is not null or 'ind_jpmc = 'Y''.

8) Combine multiple product segmentation fields

* Merge 'product_segmentation_applicable' and 'stg_codelist' into a single semicolon-separated list.

9) Cache transformed DataFrame for further use

* Resulting DataFrame 'transformed_df' now has clean column names, UUIDs, and required columns.

10) Identify JSON columns to flatten

* Using configuration in 'staging_schema', split JSON columns into 'json_to_flatten' and 'json_not_flatten'.

11) Flatten required JSON columns

* Use utility function 'flatten_json_column_improved()' to explode JSON arrays/objects into tabular columns.
* Combine all flattened outputs into a single DataFrame 'transformed_json_df'.

12) Create separate transformed DataFrames for each staging entity

* 'transformed_business_entity', 'transformed_business_entity_details', 'transformed_contacts', 'transformed_revenue', 'transformed_transaction_stability', 'transformed_characteristics', 'transformed_physical_address', etc.
* Each sub-DataFrame follows specific schema definitions from 'staging_schema.py'.

13) Perform column-specific logic and cleanup

* Apply joins, rename columns, generate UUIDs, merge data, and filter null/invalid records per entity.

14) Output ready DataFrames for writing to staging schema or S3

* These cleaned, flattened, and schema-aligned DataFrames are ready for next step (writing into staging tables).


15) Create and Access All Transformed DataFrames

* Extracts all entity DataFrames from the 'transformed_dataframes' dictionary:


16) Define 'relationship()' Function

This function builds entity relationships between buyers, suppliers, clients, and firms.

         Steps inside 'relationship()'

				1. Extract Vendor Contacts

				   * Select distinct contact info.
				   * Filter out nulls.
				   * Compute 'business_entity_contact_id' using UUID UDF.

				2. Prepare Client Mapping

				   * Broadcast small dataset of client name → 'stg_business_entity_id'.

				3. Create Three Relationship Sets

				   * Client → Firm relationships.
				   * Supplier → Client relationships.
				   * Supplier → Firm relationships.

				4. Union All Relationship Datasets

				   * Combine all three into one unified relationship DataFrame.

				5. Fix Void Data Types

				   * Replace void columns with null 'StringType()'.

				6. Generate 'business_entity_relationship_id'

				   * Compute UUID based on non-null columns.

				7. Add 'is_active' Column

				   * Mark all relationships as active.

				8. Return Final Relationship DataFrame
				

17) Generate Transformed Relationship DataFrame


* Add boolean flag 'is_active = True'.
* Fix any 'void' type columns again.
* Regenerate UUIDs for relationship IDs.
* Get distinct rows.

18) Build Relationship Subsets

* Create Supplier–Buyer dataset and Client–Firm dataset by filtering based on roles.
* Add missing columns ('client_vendor_id', etc.) for schema compatibility.
* Union both datasets to form 'transformed_rel_identifiers'.

19) Melt Relationship Identifiers

* Use 'utl.melt_dataframe()' to unpivot identifier columns into name/value pairs.
* Converts wide columns ('client_vendor_id', 'client_vendor_site_id', 'supplier_ecid') into key-value structure.
* Create additional identifiers from payor relationships ('stg_payor_business_entity_id', 'client_ecid').
* Union both sets of melted identifiers → 'transformed_rel_identifiers'.

20) Join with Relationship Info

* Join melted identifiers with 'transformed_relationship' on 'business_entity_relationship_id'.
* Select only key columns ('identifier_type', 'identifier_value', 'related_identifier').

21) Extract and Melt Business Entity Identifiers

* Select identifier columns from main transformed data ('ss.identifiers' list).
* Melt them into key/value pairs using 'utl.melt_dataframe()'.
* Drop duplicates and filter valid identifier values.

22) Anti-Join to Avoid Duplicates

* Remove overlapping identifier pairs already present in relationships.
* Append relationship identifiers to business identifiers (union).

23) Normalize Identifier Types

* Convert specific identifier names (like 'client_ecid' or 'supplier_ecid') to a common 'ecid'.

24) Generate Final UUID for Each Identifier

* Compute UUID for 'identifier_id' using all current columns.
* Add 'is_active = True'.

25) Final Data Cleaning and Schema Alignment

For each entity DataFrame:

* Filter out null key columns.
* Apply final column selection using 'selectExpr(*ss.<schema>)'.
* Ensure all columns align with staging schema (as defined in 'staging_schema.py').

This is done for:

* Business Entity
* Details
* Contacts
* Revenue
* Transaction Stability
* Payment Attributes
* Addresses (Telecom/Electronic/Physical)
* Restrictions
* Industry Classification
* Characteristics
* Card Association
* Spend Analysis
* Relationships
* Identifiers

26) Prepare Mapping for Table Loading

* Create dictionary: '{table_name: transformed_dataframe}'


27) Truncate Existing Tables in RDS

28) Load Transformed DataFrames to PostgreSQL

29) Commit Glue Job


All source (landing) data is:

* Cleaned
* Flattened
* UUID-assigned
* Relationship-mapped
* Schema-aligned
* Loaded into PostgreSQL staging schema



