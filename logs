def flatten_array_with_double_explode(df, parsed_col_name, schema, prefix, base_columns):
    """
    Handle arrays with nested arrays (like associated_people with titles)
    Output: associated_people__name, associated_people__titles
    """
    df_exploded = (
        df
        .filter(size(col(parsed_col_name)) > 0)  # Ignore empty list
        .select(*base_columns, explode(col(parsed_col_name)).alias("person"))
    )

    df_final = (
        df_exploded
        .withColumn("title", explode(col("person.titles")))
        .select(
            *base_columns,
            col("person.name").alias(f"{prefix}__name"),
            col("title").alias(f"{prefix}__titles")
        )
    )
    return df_final



from pyspark.sql.functions import regexp_replace, from_json, ArrayType, StringType, size, lit, concat_ws, when, col
from functools import reduce
import utils as utl
import staging_schema as ss

base_col = "stg_business_entity_id"
flattened_dfs = {}

# Loop through JSON_FIELD_CONFIGS for struct & array_double_explode fields
for col_name, config in ss.JSON_FIELD_CONFIGS.items():
    df_flat = utl.flatten_json_column_improved(transformed_df, col_name, config, base_columns=[base_col])
    if df_flat is not None and df_flat.count() > 0:
        flattened_dfs[col_name] = df_flat

# Handle registered_agents separately (simple array of strings)
reg_agents_df = (
    transformed_df
    .filter(~utl.is_empty_or_null("registered_agents"))
    .withColumn("registered_agents_json", regexp_replace(col("registered_agents"), "'", "\""))
    .withColumn("registered_agents", from_json(col("registered_agents_json"), ArrayType(StringType())))
    .withColumn(
        "registered_agents",
        when(size(col("registered_agents")) > 0, concat_ws(";", col("registered_agents"))).otherwise(lit(None))
    )
    .select(base_col, "registered_agents")
)

if reg_agents_df.count() > 0:
    flattened_dfs["registered_agents"] = reg_agents_df

# Combine all results
final_df = utl.combine_flattened_results(transformed_df.select(base_col).distinct(), flattened_dfs)

# Write to S3
write_single_csv_to_s3_fixed(
    final_df,
    s3_bucket="app-id-111597-dep-id-114116-uu-id-9x0jt94siuto",
    s3_key="pfd_scripts/pfd_staging_pvr_test/all_flattened_combined.csv",
    temp_path="s3://app-id-111597-dep-id-114116-uu-id-9x0jt94siuto/upsert/pfd_staging/_tmp"
)
