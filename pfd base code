import sys
import boto3
import json
import hashlib
import uuid
import psycopg2
import utils as utl
import staging_schema as ss
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col, explode, lit, when, udf, to_date, current_date
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, IntegerType, MapType
from pyspark.sql.utils import AnalysisException
from datetime import datetime
from pyspark.sql.functions import when, lit, col, explode,from_json
from pyspark.sql.functions import  concat_ws,current_timestamp






## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)







s3 = boto3.client("s3")
#s3.download_file(s3_bucket,ssl_cert_s3_path,local_ssl_cert_path)


rds_client = boto3.client ("rds",region_name=ss.region)
rds_token = rds_client.generate_db_auth_token(
    DBHostname = ss.rds_host,
    Port = ss.rds_port,
    DBUsername = ss.db_user
    )
    
pg_connection_options = {
    "url" : f"jdbc:postgresql://{ss.rds_host}:{ss.rds_port}/{ss.db_name}?sslmode=require&sslrootcert={ss.ssl_cert_s3_path}",
    "user" : ss.db_user,
    "password" : rds_token
   # "db_table":source_table
    }
    



# Register the UDF
compute_uuid_udf = udf(utl.compute_uuid, StringType())
    


    


truncate = utl.truncate_tables(ss.rds_host,ss.rds_port,ss.db_name,ss.db_user,rds_token,ss.ssl_cert_s3_path)

def get_landing_data():
    try:
        # Query to fetch data
        query = "(SELECT DISTINCT * FROM sds_landing.analysis_pfd WHERE filter_employee = 'N') AS temp_table"

        # Read the query result into a Spark DataFrame
        source_df = glueContext.read.format("jdbc").options(
                dbtable=query,
                **pg_connection_options
            ).load()

        # Define UDFs for computing business entity ID and buying entity ID
        
        

        source_df = source_df.withColumn(
                        "stg_business_entity_id",
                        when(
                            col("sds_supplier_id").isNotNull(), col("sds_supplier_id")
                            ).otherwise(
                                compute_uuid_udf(concat_ws(",", col("vendor_name_cleaned")))
                            )
                            )
                

                    
        print("stg_business_entity_id column added.")


        source_df = source_df.withColumn(
                "stg_buying_entity",
            compute_uuid_udf(concat_ws(",", col("buying_entity")))
                )
                
        print("stg_buying_entity column added.")

        print("After adding the UUID columns")
        source_df.select("stg_business_entity_id").show()
        # Return the Spark DataFrame
        return source_df

    except Exception as e:
        print('Error Message is:', e)
        return None
        


def process_mapping_and_transform_data(source_df, mapping_file_path, output_data_path):
    """
    Reads a mapping file and input data, applies column transformations, and writes the transformed data to S3.
    """
    # Read the mapping file into a Glue DynamicFrame
    mapping_df = glueContext.create_dynamic_frame.from_options(
        connection_type="s3",
        connection_options={"paths": [mapping_file_path]},
        format="csv",
        format_options={"withHeader": True}
    ).toDF()

    print("Reading mapping file completed")

    # Validate mapping DataFrame
    if mapping_df.rdd.isEmpty():
        raise ValueError("The mapping DataFrame is empty. Please check the mapping file.")
    if "source_column" not in mapping_df.columns or "target_column" not in mapping_df.columns:
        raise ValueError("The mapping DataFrame does not contain the required columns: 'source_column' and 'target_column'.")

    # Initialize variables
    column_mapping = {}
    current_date = datetime.today().strftime("%Y-%m-%d")

    # Iterate through the mapping DataFrame to create the column mapping dictionary
    for row in mapping_df.collect():
        source_col = str(row["source_column"]).strip().lower()
        target_col = row["target_column"]
        column_mapping[source_col] = target_col

   # print("Column Mapping Dictionary:")
   # print(column_mapping)

    # Read the input data into a Glue DynamicFrame
    source_df.show()
    data = source_df
    print("data:", data.show())

    # Validate input DataFrame
    if data.rdd.isEmpty():
      #  raise ValueError("The input DataFrame is empty. Please check the source data.")
      print("The input DataFrame is empty. Please check the source data.")

    # Verify that all source columns exist in the input DataFrame
    # missing_columns = [col for col in column_mapping.keys() if col not in data.columns]
    # if missing_columns:
    #     raise ValueError(f"The following columns are missing in the input data: {missing_columns}")
    #  #   print("The following columns are missing in the input data",missing_columns )
        
    print("before transforming the data")

    # Rename columns in the input DataFrame using the column mapping
    transformed_df = data.select(
        [col(c).alias(column_mapping[c]) if c in column_mapping else col(c) for c in data.columns]
    )
    # Add a new column "stg_jpmc_business_entity_id" based on the condition
    transformed_df = transformed_df.withColumn(
        "stg_jpmc_business_entity_id",
        when(
            (col("client_ecid").isNotNull()) | (col("ind_jpmc") == 'Y'),
            lit("2d35a04a-5fdf-50d5-7750-c1c7621ddc33")
        ).otherwise(None)
    )
    
    print("after transforming the data")

    print("Transformed DataFrame:")
    #transformed_df.show()

    # Validate transformed DataFrame
    if transformed_df.rdd.isEmpty():
        raise ValueError("The transformed DataFrame is empty after applying column mappings.")

    # Convert the Spark DataFrame to a Glue DynamicFrame
    #transformed_dynamic_frame = DynamicFrame.fromDF(transformed_df, glueContext, "transformed_dynamic_frame")


    

    print("Transformation and writing completed")
    return transformed_df
    
    
landing_tables= get_landing_data()
landing_tables.show()
transformed_df = process_mapping_and_transform_data(landing_tables,ss.mapping_file_path, ss.output_data_path)


from pyspark.sql import DataFrame
import os

def write_single_csv_to_s3(df: DataFrame, s3_bucket: str, s3_key: str, temp_path: str):
    """
    Writes a Spark DataFrame as a single CSV file with a specific name to S3.

    Args:
        df (DataFrame): The Spark DataFrame.
        s3_bucket (str): The S3 bucket name (without `s3://`).
        s3_key (str): The full S3 key for the final CSV file (e.g. `folder/my_file.csv`).
        temp_path (str): Temporary S3 folder (e.g. `s3://my-bucket/tmp-folder/`)
    """
    # Coalesce to one partition and write to temp S3 directory
    df.coalesce(1).write.mode("overwrite").option("header", True).csv(temp_path)

    # Initialize boto3
    s3 = boto3.client("s3")

    # Extract bucket and prefix from temp path
    temp_bucket = temp_path.replace("s3://", "").split("/")[0]
    temp_prefix = "/".join(temp_path.replace("s3://", "").split("/")[1:])

    # List objects in temp S3 folder to find the part file
    objects = s3.list_objects_v2(Bucket=temp_bucket, Prefix=temp_prefix)
    part_file = next(
        obj['Key'] for obj in objects.get('Contents', []) if obj['Key'].endswith(".csv")
    )

    # Copy part file to desired S3 location with custom name
    s3.copy_object(
        Bucket=s3_bucket,
        CopySource={'Bucket': temp_bucket, 'Key': part_file},
        Key=s3_key
    )

    # Optionally delete temp files
    for obj in objects.get('Contents', []):
        s3.delete_object(Bucket=temp_bucket, Key=obj['Key'])

# Usage:
write_single_csv_to_s3(
    transformed_df,
    s3_bucket="app-id-111597-dep-id-114116-uu-id-by081rbjj1vo",
    s3_key="upsert/pfd_staging/transformed_df.csv",
    temp_path="s3://app-id-111597-dep-id-114116-uu-id-by081rbjj1vo/upsert/pfd_staging/_tmp"
)

write_single_csv_to_s3(
    landing_tables,
    s3_bucket="app-id-111597-dep-id-114116-uu-id-by081rbjj1vo",
    s3_key="upsert/pfd_staging/source_df.csv",
    temp_path="s3://app-id-111597-dep-id-114116-uu-id-by081rbjj1vo/upsert/pfd_staging/_tmp"
)
