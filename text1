from pyspark.sql import DataFrame
import boto3
import os

def write_single_csv_to_s3(df: DataFrame, s3_bucket: str, s3_key: str, temp_path: str):
    """
    Writes a Spark DataFrame as a single CSV file with a specific name to S3.

    Args:
        df (DataFrame): The Spark DataFrame.
        s3_bucket (str): The S3 bucket name (without `s3://`).
        s3_key (str): The full S3 key for the final CSV file (e.g. `folder/my_file.csv`).
        temp_path (str): Temporary S3 folder (e.g. `s3://my-bucket/tmp-folder/`)
    """
    # Coalesce to one partition and write to temp S3 directory
    df.coalesce(1).write.mode("overwrite").option("header", True).csv(temp_path)

    # Initialize boto3
    s3 = boto3.client("s3")

    # Extract bucket and prefix from temp path
    temp_bucket = temp_path.replace("s3://", "").split("/")[0]
    temp_prefix = "/".join(temp_path.replace("s3://", "").split("/")[1:])

    # List objects in temp S3 folder to find the part file
    objects = s3.list_objects_v2(Bucket=temp_bucket, Prefix=temp_prefix)
    part_file = next(
        obj['Key'] for obj in objects.get('Contents', []) if obj['Key'].endswith(".csv")
    )

    # Copy part file to desired S3 location with custom name
    s3.copy_object(
        Bucket=s3_bucket,
        CopySource={'Bucket': temp_bucket, 'Key': part_file},
        Key=s3_key
    )

    # Optionally delete temp files
    for obj in objects.get('Contents', []):
        s3.delete_object(Bucket=temp_bucket, Key=obj['Key'])

# Usage:
write_single_csv_to_s3(
    transformed_df,
    s3_bucket="app-id-111597-dep-id-114116-uu-id-by081rbjj1vo",
    s3_key="upsert/pfd_staging/transformed_df.csv",
    temp_path="s3://app-id-111597-dep-id-114116-uu-id-by081rbjj1vo/upsert/pfd_staging/_tmp"
)
