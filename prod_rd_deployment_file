import os
import sys
import findspark

# Ensure the current Python interpreter is used
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# Initialize PySpark
findspark.init()

from pyspark.sql import SparkSession

# Create a minimal SparkSession
print("Creating minimal Spark session...")
spark = SparkSession.builder \
    .appName("Minimal PySpark Test") \
    .master("local[1]") \
    .config("spark.driver.memory", "1g") \
    .config("spark.executor.memory", "1g") \
    .getOrCreate()

# Print Spark version
print(f"Successfully created Spark session. Spark version: {spark.version}")

# Create a very simple DataFrame with just one row
print("Creating minimal DataFrame...")
data = [("Test", 1)]
columns = ["Value", "Number"]
df = spark.createDataFrame(data, columns)

# Show the DataFrame - simplest operation
print("DataFrame content:")
df.show()

# Stop the SparkSession
print("Stopping Spark session...")
spark.stop()
print("Spark session stopped. Script completed successfully!")
