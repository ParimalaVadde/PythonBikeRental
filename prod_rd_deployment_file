from pyspark.sql import SparkSession
import psycopg2
from psycopg2.extras import execute_batch
import pandas as pd
from urllib.parse import quote_plus
from datetime import date

def sync_dataframe_with_postgresql(spark_df, connection_params, table_name, compare_columns, schema_name, column_mapping, load_flag_column):
    """
    Syncs a PySpark DataFrame with a PostgreSQL table by comparing records and performing insert or update operations
    based on the load_flag column.

    :param spark_df: PySpark DataFrame containing the data to sync
    :param connection_params: Dictionary with PostgreSQL connection parameters
    :param table_name: Name of the PostgreSQL table
    :param compare_columns: List of column names to compare for identifying existing records
    :param schema_name: Schema name of the PostgreSQL table
    :param column_mapping: Dictionary mapping DataFrame column names to PostgreSQL table column names
    :param load_flag_column: Column name containing the load flag ('I' for insert, 'U' for update)
    """
    # Apply column mapping (if provided)
    if column_mapping is None:
        column_mapping = {}
    mapped_columns = {col: column_mapping.get(col, col) for col in spark_df.columns}
    
    # Convert PySpark DataFrame to Pandas for easier handling with psycopg2
    # Note: In a production environment with large datasets, consider processing in batches
    # or using JDBC directly with Spark
    pandas_df = spark_df.toPandas()
    
    # Replace NaN values with None (to handle NULL in PostgreSQL)
    pandas_df = pandas_df.where(pd.notna(pandas_df), None)  # Convert all Numpy NaN values to None
    pandas_df = pandas_df.applymap(lambda x: None if isinstance(x, str) and x.strip().lower() == "nan" else x)  # Replace case-insensitive "nan" with None
    pandas_df = pandas_df.replace(r'^\s*$', None, regex=True)  # Replace empty or whitespace-only strings with None
    pandas_df = pandas_df.astype(object).where(pd.notna(pandas_df), None)  # Convert dataframe to python native types
    
    # Connect to PostgreSQL
    conn = psycopg2.connect(
        dbname=connection_params['database'],
        user=connection_params['user'],
        password=connection_params['password'],
        host=connection_params['host'],
        port=connection_params['port']
    )
    cursor = conn.cursor()
    
    # Get the primary keys of the table
    cursor.execute(f"""
        SELECT a.attname
        FROM pg_index i
        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
        WHERE i.indrelid = '{schema_name}.{table_name}'::regclass
        AND i.indisprimary;
    """)
    primary_keys = [row[0] for row in cursor.fetchall()]
    print(f"Primary keys for table {table_name}: {primary_keys}")
    
    # List to store mismatched records
    mismatched_records = []
    
    # Process each row
    for _, row in pandas_df.iterrows():
        row_dict = row.to_dict()
        load_flag = row_dict.get(load_flag_column)
        
        try:
            if load_flag and load_flag.upper() == "U":
                # Create WHERE clause for comparison
                where_clause = " AND ".join([f"{mapped_columns[col]} = %s" for col in compare_columns])
                where_values = [row_dict[col] for col in compare_columns]
                
                # Check if record exists
                cursor.execute(f"SELECT 1 FROM {schema_name}.{table_name} WHERE {where_clause}", where_values)
                record_exists = cursor.fetchone() is not None
                
                if record_exists:
                    # Prepare update data
                    update_data = {
                        mapped_columns[col]: row_dict[col]
                        for col in pandas_df.columns
                        if col not in compare_columns and mapped_columns[col] not in primary_keys and col != load_flag_column and col != "created_by"
                    }
                    update_data["last_updated_by"] = row_dict["created_by"]
                    update_data["last_updated_date"] = date.today()
                    
                    # Build SET clause
                    set_clause = ", ".join([f"{col} = %s" for col in update_data.keys()])
                    set_values = list(update_data.values())
                    
                    # Execute update
                    cursor.execute(
                        f"UPDATE {schema_name}.{table_name} SET {set_clause} WHERE {where_clause}",
                        set_values + where_values
                    )
                else:
                    # Record doesn't exist for update
                    mismatched_records.append(row_dict)
                    
            elif load_flag and load_flag.upper() == "I":
                # Check if record exists
                where_clause = " AND ".join([f"{mapped_columns[col]} = %s" for col in compare_columns])
                where_values = [row_dict[col] for col in compare_columns]
                
                cursor.execute(f"SELECT 1 FROM {schema_name}.{table_name} WHERE {where_clause}", where_values)
                record_exists = cursor.fetchone() is not None
                
                if not record_exists:
                    # Prepare insert data
                    insert_data = {
                        mapped_columns[col]: row_dict[col]
                        for col in pandas_df.columns
                        if col != load_flag_column
                    }
                    insert_data["is_active"] = True
                    insert_data["last_updated_by"] = insert_data["created_by"]
                    insert_data["last_updated_date"] = date.today()
                    
                    # Build INSERT statement
                    columns = ", ".join(insert_data.keys())
                    placeholders = ", ".join(["%s"] * len(insert_data))
                    values = list(insert_data.values())
                    
                    # Execute insert
                    cursor.execute(
                        f"INSERT INTO {schema_name}.{table_name} ({columns}) VALUES ({placeholders})",
                        values
                    )
                else:
                    # Record already exists for insert
                    mismatched_records.append(row_dict)
            else:
                # Invalid load_flag
                mismatched_records.append(row_dict)
                
        except Exception as e:
            print(f"Error processing record: {row_dict}, Error: {e}")
            mismatched_records.append(row_dict)
    
    # Commit changes and close connection
    conn.commit()
    cursor.close()
    conn.close()
    
    # Write mismatched records to a CSV file
    if mismatched_records:
        mismatched_df = pd.DataFrame(mismatched_records)
        mismatched_df.to_csv("mismatch_records.csv", index=False)
        print("Mismatched records written to mismatch_records.csv")

def main():
    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("PostgreSQL Data Sync") \
        .getOrCreate()
    
    # Load data from CSV
    stage_file_path = r"C:/Users/w619378/Downloads/b2b payables/upsert logic/card_revenues.csv"
    
    # Read CSV into Spark DataFrame
    spark_df = spark.read.option("header", "true").csv(stage_file_path)
    
    # Print dataframe schema
    spark_df.printSchema()
    spark_df.show(5)
    
    # Define the PostgreSQL connection parameters
    db_config = {
        'user': 'xbsddevdbAuroraAppAdmin',
        'password': 'rdsapg86a8-auroratdb-v1-node-1.chzdhpfung6p.us-east-1.rds.amazonaws.com:6160/?Action=connect&DBUser=xbsddevdbAuroraAppAdmin&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAXBZAIY4B5FD7UTBN%2F20250508%2Fus-east-1%2Frds-db%2Faws4_request&X-Amz-Date=20250508T092547Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIBrajVBFFE7xcZgnPbdEueFVIFVEqBMqao0LPV7xwmnlAiEAzLQTtsIMiQmIBoxanC9gVJjbCpy37frqTqiNh1H4V0gq2AIIcxAAGgw0ODQ4NjIxMTc2MzUiDNuvqEYwaEcCPEuvSSq1AoflJWZoKkBPA%2BwTzHPV4OVBmd6SXjGr7b4pqF2vz%2FVzp1rDLEm%2BVdmaXWTX8KZq%2BRjUk36l23ukLG99ZMpBLmKbnQ3Ee4Cqz8GdT3Vg6gjYMCSv40HL5EsOJzMynmmFOSVu%2B78s46rjImv99NprmHQR0J6aLZLcTm1Q21bOiW2QQpWRi0z6VVqvBvG39PX%2Ba6Wf2mhtEvEaQDdupKBnu%2BObba7KAtz65rXVnK%2Fs15FnjYqMBpDbx7%2BduUORvk03sv4KMVxLVeHPp7Ats5LfuuhdODq6o4QkA%2BSormL3XAtfQIglJCQra2CqPo%2FbnS9rc0CS1aoRcHade6cd9kLzC7TnBsujSfQCzjH%2FkejsrmG7LVa3eaydd%2FcVGErJOz7vuDODiHf8C0i4VsZkxcIGPNFO4bJSNzCJ8fHABjqiAYSP5PjILETmZwhF8YVKvtxHHkOmaCPhMw0VHiP5GqAHG0pndlmgjrRbwlkcHgbpzyplzsYdP7V4IhM6UkZiWZTNIAziOUVEjJ8OJuQK1zP%2F1MNDdvagypujxF7PZeEdenBFXTsLvnSOH59o8Jm3P5GSo5rK6K0vBW%2FT8T4mOlC49YPb7prNj%2BG8LF2VoHGyhyt2I6yGFGfWhMLexf%2FZncsYuQ%3D%3D&X-Amz-Signature=85808d1b361693036002b72421a19864f94504bf31b59b793c8d835f60775b7b',
        'host': 'nlb-rdsapg86a8-auroratdb-v1-fdcc221bbc9eea60.elb.us-east-1.amazonaws.com',
        'port': '6160',
        'database': 'xbsddevdb',
    }
    
    # For large datasets, you could use Spark JDBC directly:
    # jdbc_url = f"jdbc:postgresql://{db_config['host']}:{db_config['port']}/{db_config['database']}"
    # properties = {
    #     "user": db_config['user'],
    #     "password": db_config['password'],
    #     "driver": "org.postgresql.Driver"
    # }
    
    schema_name = "sds_main"
    table_name = "business_entity_card_revenues"
    compare_columns = ["stg_business_entity_id", "end_date"]
    column_mapping = {  # Map only the mismatched column
        "stg_business_entity_id": "business_entity_id"
    }
    load_flag_column = "load_flag"
    
    # Call the sync function
    sync_dataframe_with_postgresql(
        spark_df, 
        db_config, 
        table_name, 
        compare_columns, 
        schema_name, 
        column_mapping, 
        load_flag_column
    )
    
    print("Sync operation completed successfully.")
    
    # Stop Spark session
    spark.stop()

if __name__ == "__main__":
    main()
