import os
import sys
import findspark
import platform

# Define the paths you need to set
# Replace these with your actual paths if different
PYTHON_PATH = sys.executable  # This will use the current Python executable
JAVA_HOME = os.environ.get("JAVA_HOME", "C:\\Program Files\\Java\\jdk1.8.0_401")  # Adjust based on your Java path
SPARK_HOME = os.path.join(os.path.dirname(findspark.__file__), "spark")  # This will attempt to find Spark

# Set key environment variables
os.environ["PYSPARK_PYTHON"] = PYTHON_PATH
os.environ["PYSPARK_DRIVER_PYTHON"] = PYTHON_PATH
os.environ["JAVA_HOME"] = JAVA_HOME
os.environ["SPARK_HOME"] = SPARK_HOME

# Set up Spark
findspark.init()

from pyspark.sql import SparkSession

# Create a SparkSession with optimized configurations for Windows
spark = SparkSession.builder \
    .appName("FixedPySparkApp") \
    .master("local[1]") \
    .config("spark.python.worker.reuse", "false") \
    .config("spark.executor.memory", "1g") \
    .config("spark.driver.memory", "1g") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
    .getOrCreate()

print("SparkSession created successfully!")

# Test if it works
test_df = spark.createDataFrame([(1, "test"), (2, "test2")], ["id", "value"])
print("Test DataFrame created successfully")
test_df.show()

# Test a UDF (this uses Python workers)
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

@udf(returnType=StringType())
def test_udf(value):
    return f"Processed: {value}"

result_df = test_df.withColumn("processed", test_udf(test_df.value))
print("UDF applied successfully")
result_df.show()

print("All tests passed successfully!")
