from pyspark.sql import SparkSession, Row, Window
from pyspark.sql.functions import col, lit, current_date, when, trim, upper, length
import psycopg2
from psycopg2.extras import execute_batch
from urllib.parse import quote_plus
import json

def sync_dataframe_with_postgresql(spark_df, connection_params, table_name, compare_columns, schema_name, column_mapping, load_flag_column):
    """
    Syncs a PySpark DataFrame with a PostgreSQL table by comparing records and performing insert or update operations
    based on the load_flag column.

    :param spark_df: PySpark DataFrame containing the data to sync
    :param connection_params: Dictionary with PostgreSQL connection parameters
    :param table_name: Name of the PostgreSQL table
    :param compare_columns: List of column names to compare for identifying existing records
    :param schema_name: Schema name of the PostgreSQL table
    :param column_mapping: Dictionary mapping DataFrame column names to PostgreSQL table column names
    :param load_flag_column: Column name containing the load flag ('I' for insert, 'U' for update)
    """
    # Apply column mapping (if provided)
    if column_mapping is None:
        column_mapping = {}
    
    # Clean the data
    # Replace empty strings and "nan" strings with None/null
    for column in spark_df.columns:
        spark_df = spark_df.withColumn(
            column,
            when((trim(col(column)) == "") | 
                 (trim(upper(col(column))) == "NAN"), None)
            .otherwise(col(column))
        )
    
    # Get database table structure
    conn = psycopg2.connect(
        dbname=connection_params['database'],
        user=connection_params['user'],
        password=connection_params['password'],
        host=connection_params['host'],
        port=connection_params['port']
    )
    cursor = conn.cursor()
    
    # Get the primary keys of the table
    cursor.execute(f"""
        SELECT a.attname
        FROM pg_index i
        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
        WHERE i.indrelid = '{schema_name}.{table_name}'::regclass
        AND i.indisprimary;
    """)
    primary_keys = [row[0] for row in cursor.fetchall()]
    print(f"Primary keys for table {table_name}: {primary_keys}")
    
    # Create temp tables for batch operations
    full_table_name = f"{schema_name}.{table_name}"
    
    # Get column information for target table
    cursor.execute(f"""
        SELECT column_name, data_type 
        FROM information_schema.columns 
        WHERE table_schema = '{schema_name}' 
        AND table_name = '{table_name}'
    """)
    columns_info = {row[0]: row[1] for row in cursor.fetchall()}
    
    # Split the dataframe by load flag
    update_df = spark_df.filter(upper(col(load_flag_column)) == "U")
    insert_df = spark_df.filter(upper(col(load_flag_column)) == "I")
    invalid_df = spark_df.filter((upper(col(load_flag_column)) != "U") & (upper(col(load_flag_column)) != "I"))
    
    # Process records for update
    mismatched_updates = []
    if not update_df.isEmpty():
        # Create a temporary table for updates
        temp_update_table = f"temp_update_{table_name}"
        cursor.execute(f"DROP TABLE IF EXISTS {schema_name}.{temp_update_table}")
        
        # Create temp table with same structure as target
        cursor.execute(f"CREATE TABLE {schema_name}.{temp_update_table} AS SELECT * FROM {full_table_name} LIMIT 0")
        conn.commit()
        
        # Write update candidates to PostgreSQL for batch processing
        update_jdbc_url = f"jdbc:postgresql://{connection_params['host']}:{connection_params['port']}/{connection_params['database']}"
        update_properties = {
            "user": connection_params['user'],
            "password": connection_params['password'],
            "driver": "org.postgresql.Driver"
        }
        
        # Prepare update dataframe without load_flag
        update_df_prepared = update_df.drop(load_flag_column)
        
        # Write to temp table
        update_df_prepared.write \
            .option("truncate", "true") \
            .jdbc(
                url=update_jdbc_url,
                table=f"{schema_name}.{temp_update_table}",
                mode="overwrite",
                properties=update_properties
            )
        
        # Get existing records based on compare columns for updates
        compare_conditions = " AND ".join([f"a.{mapped_columns.get(col, col)} = b.{mapped_columns.get(col, col)}" 
                                         for col in compare_columns])
        
        update_columns = [col for col in update_df.columns 
                         if col not in compare_columns 
                         and mapped_columns.get(col, col) not in primary_keys 
                         and col != load_flag_column 
                         and col != "created_by"]
        
        if update_columns:
            set_clause = ", ".join([f"{mapped_columns.get(col, col)} = b.{mapped_columns.get(col, col)}" 
                                   for col in update_columns])
            
            set_clause += ", last_updated_by = b.created_by, last_updated_date = CURRENT_DATE"
            
            # Execute batch update
            cursor.execute(f"""
                UPDATE {full_table_name} a
                SET {set_clause}
                FROM {schema_name}.{temp_update_table} b
                WHERE {compare_conditions}
            """)
            
            # Find records that should be updated but don't exist
            cursor.execute(f"""
                CREATE TEMP TABLE missing_updates AS
                SELECT b.*
                FROM {schema_name}.{temp_update_table} b
                LEFT JOIN {full_table_name} a
                ON {compare_conditions}
                WHERE a.{primary_keys[0]} IS NULL
            """)
            
            # Get the mismatched records
            cursor.execute("SELECT * FROM missing_updates")
            mismatched_updates = cursor.fetchall()
            
            # Clean up
            cursor.execute("DROP TABLE missing_updates")
            cursor.execute(f"DROP TABLE {schema_name}.{temp_update_table}")
            conn.commit()
    
    # Process records for insert
    mismatched_inserts = []
    if not insert_df.isEmpty():
        # Create a temporary table for inserts
        temp_insert_table = f"temp_insert_{table_name}"
        cursor.execute(f"DROP TABLE IF EXISTS {schema_name}.{temp_insert_table}")
        
        # Create temp table with same structure as target
        cursor.execute(f"CREATE TABLE {schema_name}.{temp_insert_table} AS SELECT * FROM {full_table_name} LIMIT 0")
        conn.commit()
        
        # Write insert candidates to PostgreSQL for batch processing
        insert_jdbc_url = f"jdbc:postgresql://{connection_params['host']}:{connection_params['port']}/{connection_params['database']}"
        insert_properties = {
            "user": connection_params['user'],
            "password": connection_params['password'],
            "driver": "org.postgresql.Driver"
        }
        
        # Prepare insert dataframe without load_flag and with additional required fields
        insert_df_prepared = insert_df.drop(load_flag_column) \
            .withColumn("is_active", lit(True)) \
            .withColumn("last_updated_by", col("created_by")) \
            .withColumn("last_updated_date", current_date())
        
        # Apply column mapping for JDBC
        for original_col, mapped_col in column_mapping.items():
            if original_col in insert_df_prepared.columns:
                insert_df_prepared = insert_df_prepared.withColumnRenamed(original_col, mapped_col)
        
        # Write to temp table
        insert_df_prepared.write \
            .option("truncate", "true") \
            .jdbc(
                url=insert_jdbc_url,
                table=f"{schema_name}.{temp_insert_table}",
                mode="overwrite",
                properties=insert_properties
            )
        
        # Get existing records based on compare columns for inserts
        compare_conditions_insert = " AND ".join([f"a.{mapped_columns.get(col, col)} = b.{mapped_columns.get(col, col)}" 
                                               for col in compare_columns])
        
        # Find records that don't exist yet (for insert)
        cursor.execute(f"""
            CREATE TEMP TABLE valid_inserts AS
            SELECT b.*
            FROM {schema_name}.{temp_insert_table} b
            LEFT JOIN {full_table_name} a
            ON {compare_conditions_insert}
            WHERE a.{primary_keys[0]} IS NULL
        """)
        
        # Get column list for insertion
        cursor.execute(f"SELECT column_name FROM information_schema.columns WHERE table_schema = '{schema_name}' AND table_name = '{temp_insert_table}'")
        insert_columns = [row[0] for row in cursor.fetchall()]
        columns_str = ", ".join(insert_columns)
        
        # Insert valid records
        cursor.execute(f"""
            INSERT INTO {full_table_name} ({columns_str})
            SELECT {columns_str} FROM valid_inserts
        """)
        
        # Find records that already exist (these are mismatches)
        cursor.execute(f"""
            CREATE TEMP TABLE duplicate_inserts AS
            SELECT b.*
            FROM {schema_name}.{temp_insert_table} b
            JOIN {full_table_name} a
            ON {compare_conditions_insert}
        """)
        
        # Get the mismatched records
        cursor.execute("SELECT * FROM duplicate_inserts")
        mismatched_inserts = cursor.fetchall()
        
        # Clean up
        cursor.execute("DROP TABLE valid_inserts")
        cursor.execute("DROP TABLE duplicate_inserts")
        cursor.execute(f"DROP TABLE {schema_name}.{temp_insert_table}")
        conn.commit()
    
    # Collect invalid and mismatched records into a single Spark DataFrame
    mismatched_records = []
    
    # Convert mismatched updates to Row objects
    if mismatched_updates:
        column_names = [desc[0] for desc in cursor.description]
        for record in mismatched_updates:
            mismatched_records.append(dict(zip(column_names, record)))
    
    # Convert mismatched inserts to Row objects
    if mismatched_inserts:
        column_names = [desc[0] for desc in cursor.description]
        for record in mismatched_inserts:
            mismatched_records.append(dict(zip(column_names, record)))
    
    # Add invalid records
    if not invalid_df.isEmpty():
        for row in invalid_df.collect():
            mismatched_records.append(row.asDict())
    
    # Create mismatched DataFrame and write to CSV
    if mismatched_records:
        mismatched_schema = spark_df.schema
        mismatched_df = spark.createDataFrame(mismatched_records)
        mismatched_df.write.mode("overwrite").option("header", "true").csv("mismatch_records")
        print("Mismatched records written to mismatch_records directory")
    
    # Close database connection
    cursor.close()
    conn.close()

def main():
    # Initialize Spark session
    global spark
    spark = SparkSession.builder \
        .appName("PostgreSQL Data Sync") \
        .config("spark.jars", "/path/to/postgresql-42.5.1.jar")  # PostgreSQL JDBC driver path
        .getOrCreate()
    
    # Load data from CSV
    stage_file_path = r"C:/Users/w619378/Downloads/b2b payables/upsert logic/card_revenues.csv"
    
    # Read CSV into Spark DataFrame
    spark_df = spark.read.option("header", "true").csv(stage_file_path)
    
    # Print dataframe schema
    spark_df.printSchema()
    spark_df.show(5)
    
    # Define the PostgreSQL connection parameters
    db_config = {
        'user': 'xbsddevdbAuroraAppAdmin',
        'password': 'rdsapg86a8-auroratdb-v1-node-1.chzdhpfung6p.us-east-1.rds.amazonaws.com:6160/?Action=connect&DBUser=xbsddevdbAuroraAppAdmin&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAXBZAIY4B5FD7UTBN%2F20250508%2Fus-east-1%2Frds-db%2Faws4_request&X-Amz-Date=20250508T092547Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIBrajVBFFE7xcZgnPbdEueFVIFVEqBMqao0LPV7xwmnlAiEAzLQTtsIMiQmIBoxanC9gVJjbCpy37frqTqiNh1H4V0gq2AIIcxAAGgw0ODQ4NjIxMTc2MzUiDNuvqEYwaEcCPEuvSSq1AoflJWZoKkBPA%2BwTzHPV4OVBmd6SXjGr7b4pqF2vz%2FVzp1rDLEm%2BVdmaXWTX8KZq%2BRjUk36l23ukLG99ZMpBLmKbnQ3Ee4Cqz8GdT3Vg6gjYMCSv40HL5EsOJzMynmmFOSVu%2B78s46rjImv99NprmHQR0J6aLZLcTm1Q21bOiW2QQpWRi0z6VVqvBvG39PX%2Ba6Wf2mhtEvEaQDdupKBnu%2BObba7KAtz65rXVnK%2Fs15FnjYqMBpDbx7%2BduUORvk03sv4KMVxLVeHPp7Ats5LfuuhdODq6o4QkA%2BSormL3XAtfQIglJCQra2CqPo%2FbnS9rc0CS1aoRcHade6cd9kLzC7TnBsujSfQCzjH%2FkejsrmG7LVa3eaydd%2FcVGErJOz7vuDODiHf8C0i4VsZkxcIGPNFO4bJSNzCJ8fHABjqiAYSP5PjILETmZwhF8YVKvtxHHkOmaCPhMw0VHiP5GqAHG0pndlmgjrRbwlkcHgbpzyplzsYdP7V4IhM6UkZiWZTNIAziOUVEjJ8OJuQK1zP%2F1MNDdvagypujxF7PZeEdenBFXTsLvnSOH59o8Jm3P5GSo5rK6K0vBW%2FT8T4mOlC49YPb7prNj%2BG8LF2VoHGyhyt2I6yGFGfWhMLexf%2FZncsYuQ%3D%3D&X-Amz-Signature=85808d1b361693036002b72421a19864f94504bf31b59b793c8d835f60775b7b',
        'host': 'nlb-rdsapg86a8-auroratdb-v1-fdcc221bbc9eea60.elb.us-east-1.amazonaws.com',
        'port': '6160',
        'database': 'xbsddevdb',
    }
    
    schema_name = "sds_main"
    table_name = "business_entity_card_revenues"
    compare_columns = ["stg_business_entity_id", "end_date"]
    column_mapping = {  # Map only the mismatched column
        "stg_business_entity_id": "business_entity_id"
    }
    load_flag_column = "load_flag"
    
    # Call the sync function
    sync_dataframe_with_postgresql(
        spark_df, 
        db_config, 
        table_name, 
        compare_columns, 
        schema_name, 
        column_mapping, 
        load_flag_column
    )
    
    print("Sync operation completed successfully.")
    
    # Stop Spark session
    spark.stop()

if __name__ == "__main__":
    main()
