# Import required libraries
import os
import sys

# Set Hadoop home programmatically - uncomment if you haven't set environment variables
# os.environ['HADOOP_HOME'] = 'C:/hadoop'
# os.environ['PATH'] = f"C:/hadoop/bin;{os.environ['PATH']}"

# Initialize PySpark
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, concat

# Create a SparkSession
print("Creating Spark session...")
spark = SparkSession.builder \
    .appName("PySpark Basic Test") \
    .master("local[*]") \
    .getOrCreate()

# Print Spark version
print(f"Successfully created Spark session. Spark version: {spark.version}")

# Create a simple DataFrame
print("Creating DataFrame...")
data = [
    ("Alice", 25, "New York"),
    ("Bob", 30, "San Francisco"),
    ("Charlie", 35, "Seattle"),
    ("David", 40, "Boston"),
    ("Emma", 45, "Chicago")
]
columns = ["Name", "Age", "City"]
df = spark.createDataFrame(data, columns)

# Show the DataFrame
print("Original DataFrame:")
df.show()

# Perform some basic transformations
print("Adding a greeting column...")
df_with_greeting = df.withColumn(
    "Greeting", 
    concat(lit("Hello, "), col("Name"), lit("! You are "), col("Age"), lit(" years old."))
)

# Show the transformed DataFrame
print("DataFrame with greeting:")
df_with_greeting.show(truncate=False)

# Filter the DataFrame
print("Filtering for people over 30...")
filtered_df = df.filter(col("Age") > 30)
filtered_df.show()

# Group by and aggregate
print("Calculating average age by city...")
df.groupBy("City").avg("Age").withColumnRenamed("avg(Age)", "Average_Age").show()

# Write the DataFrame to a CSV file
print("Writing DataFrame to CSV...")
df.write.mode("overwrite").csv("spark_test_output.csv", header=True)
print("CSV file written successfully.")

# Stop the SparkSession
print("Stopping Spark session...")
spark.stop()
print("Spark session stopped. Script completed successfully!")
