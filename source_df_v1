from pyspark.sql.functions import col

#Define revenue columns that should not all be NULL
revenue_value_columns = [
    "end_date",
    "card_revenue__1m__start_date",
    "card_revenue__1m__average_monthly_amount",
    "card_revenue__3m__start_date",
    "card_revenue__3m__average_monthly_amount",
    "card_revenue__12m__start_date",
    "card_revenue__12m__average_monthly_amount"
]

#Remove duplicates
transformed_revenue = transformed_revenue.dropDuplicates()

#Build condition: At least one of these columns must be non-null
non_null_condition = None
for col_name in revenue_value_columns:
    cond = col(col_name).isNotNull()
    non_null_condition = cond if non_null_condition is None else (non_null_condition | cond)

#Filter out rows where all these revenue columns are NULL
transformed_revenue = transformed_revenue.filter(non_null_condition)


#Remove 'stg_business_entity_id'
revenue_columns = [col for col in ss.revenue if col != "stg_business_entity_id"]

#Rename "card_revenue__end_date" â†’ "end_date"
revenue_columns = ["end_date" if col == "card_revenue__end_date" else col for col in revenue_columns]
