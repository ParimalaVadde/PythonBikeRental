# Combine all results
transformed_json_df = utl.combine_flattened_results(transformed_df.select(ss.base_col).distinct(), flattened_dfs)


# Transform `transformed_revenue`
    
expected_revenue_columns = [col for col in ss.revenue if col != "stg_business_entity_id"]
for col_name in expected_revenue_columns:
    if col_name not in transformed_json_df.columns:
        print(f"Creating {col_name} column with null values")
        dtype = (
            "date" if "date" in col_name
            else "double" if "amount" in col_name
            else "string"
        )
        transformed_json_df = transformed_json_df.withColumn(col_name, lit(None).cast(dtype))

transformed_revenue = transformed_json_df.select(*ss.revenue).withColumnRenamed(
    "card_revenue__end_date", "end_date"
)

# #Remove duplicates
# transformed_revenue = transformed_revenue.dropDuplicates()

#Rename "card_revenue__end_date" â†’ "end_date"
revenue_value_columns = ["end_date" if col == "card_revenue__end_date" else col for col in ss.revenue]

#Build condition: At least one of these columns must be non-null
transformed_revenue = utl.process_flattened_df_with_select_distinct(transformed_revenue, revenue_value_columns, ss.base_col)

transformed_revenue = transformed_revenue.withColumn(
"is_active", 
lit(True)  # This creates a Boolean column with True values for all rows
)

# Get columns present in both transformed_revenue and decimal_columns
columns_to_round = [col for col in ss.decimal_columns if col in transformed_revenue.columns]

# Apply rounding to decimal columns
for col_name in columns_to_round:
    transformed_revenue = transformed_revenue.withColumn(
        col_name,
        round(col(col_name), 2)
    )

transformed_revenue = transformed_revenue.withColumn(
    "revenue_id",
    compute_uuid_udf(struct(*transformed_revenue.columns))
).dropDuplicates()




def process_flattened_df_with_select_distinct(df: DataFrame, ss_columns: list, key_col: str = "stg_business_entity_id") -> DataFrame:
    """
    Optimized version using select + distinct
    """
    if key_col in ss_columns:
        processing_columns = [c for c in ss_columns if c != key_col]
    else:
        processing_columns = ss_columns

    return (
        df
        .select(processing_columns)  # Early column pruning - KEY OPTIMIZATION
        .withColumn("_any_not_null", coalesce(*[col(c) for c in processing_columns]))
        .filter(col("_any_not_null").isNotNull())
        .drop("_any_not_null")
        .select(ss_columns)  # Final column selection (adds back key_col if needed)
        .distinct()          # Distinct on smaller dataset
        .coalesce(20)
    )
	
	
ss.revenue = ["stg_business_entity_id", "card_revenue__end_date", "card_revenue__1m__start_date", "card_revenue__1m__average_monthly_amount", "card_revenue__3m__start_date", "card_revenue__3m__average_monthly_amount", "card_revenue__12m__start_date", "card_revenue__12m__average_monthly_amount"]


error 
Error Category: QUERY_ERROR; Failed Line Number: 373; Spark Error Class: MISSING_COLUMN; AnalysisException: Column 'stg_business_entity_id' does not exist. Did you mean one of the following? [end_date, card_revenue__1m__start_date, card_revenue__3m__start_date, card_revenue__12m__start_date, card_revenue__1m__average_monthly_amount, card_revenue__3m__average_monthly_amount, card_revenue__12m__average_monthly_amount];

#Build condition: At least one of these columns must be non-null
transformed_revenue = utl.process_flattened_df_with_select_distinct(transformed_revenue, revenue_value_columns, ss.base_col)
