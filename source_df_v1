from functools import reduce
from pyspark.sql.functions import col, when, coalesce

# Define association columns
association = ["stg_business_entity_id", "match_confidence", "matched_level_2", "matched_level_3_summary", 
               "matched_level_3_lineitem", "matched_fleet_ind", "matched_data_quality", "matched_mcc", "ind_card_match"]

# Get expected columns (excluding business entity id for null filtering)
expected_association_columns = [col for col in ss.association if col != "stg_business_entity_id"]

# Build comprehensive non-null condition (checks for null, empty string, and "null" string)
individual_conditions = [
    (col(col_name).isNotNull()) & (col(col_name) != "") & (col(col_name) != "null")
    for col_name in expected_association_columns
]
# At least one column should meet all the non-null/non-empty conditions
non_null_condition = reduce(lambda a, b: a | b, individual_conditions)

# CORRECTED: Include stg_business_entity_id in select and fix method chaining
transformed_association = (transformed_df
    .select(["stg_business_entity_id"] + expected_association_columns)  # Include business entity ID
    .filter(non_null_condition)
    .distinct()
    .withColumn(
        "association_name",
        when(col("ind_card_match").isin("yes", "YES"), "visa").otherwise("")
    )
)
