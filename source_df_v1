from pyspark.sql.functions import col

#Define revenue columns that should not all be NULL
revenue_value_columns = [
    "end_date",
    "card_revenue__1m__start_date",
    "card_revenue__1m__average_monthly_amount",
    "card_revenue__3m__start_date",
    "card_revenue__3m__average_monthly_amount",
    "card_revenue__12m__start_date",
    "card_revenue__12m__average_monthly_amount"
]

#Remove duplicates
transformed_revenue = transformed_revenue.dropDuplicates()

#Build condition: At least one of these columns must be non-null
non_null_condition = None
for col_name in revenue_value_columns:
    cond = col(col_name).isNotNull()
    non_null_condition = cond if non_null_condition is None else (non_null_condition | cond)

#Filter out rows where all these revenue columns are NULL
transformed_revenue = transformed_revenue.filter(non_null_condition)
