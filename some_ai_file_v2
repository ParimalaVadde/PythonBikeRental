import sys
import logging
from datetime import date
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
import boto3
import psycopg2
from pyspark.sql import functions as F
from pyspark.sql.types import BooleanType, StringType, DateType
import payload as ss  # Import sensitive configuration

# Initialize Spark and Glue contexts
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Initialize logger
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

def get_db_connection():
    """
    Establishes a PostgreSQL connection using AWS RDS authentication.
    """
    try:
        rds_client = boto3.client("rds", region_name=ss.region)
        
        # Generate database authentication token
        rds_token = rds_client.generate_db_auth_token(
            DBHostname=ss.rds_host,
            Port=ss.rds_port,
            DBUsername=ss.db_user
        )
        
        # PostgreSQL connection details
        pg_connection = psycopg2.connect(
            host=ss.rds_host,
            port=ss.rds_port,
            dbname=ss.db_name,
            user=ss.db_user,
            password=rds_token
        )
        
        logger.info("Successfully established database connection")
        return pg_connection
    except Exception as e:
        logger.error(f"Error establishing database connection: {e}")
        raise

def get_jdbc_url():
    """
    Returns a JDBC URL for PostgreSQL connection.
    """
    rds_client = boto3.client("rds", region_name=ss.region)
    
    # Generate database authentication token
    rds_token = rds_client.generate_db_auth_token(
        DBHostname=ss.rds_host,
        Port=ss.rds_port,
        DBUsername=ss.db_user
    )
    
    return f"jdbc:postgresql://{ss.rds_host}:{ss.rds_port}/{ss.db_name}?sslmode=require", ss.db_user, rds_token

def get_primary_keys(schema_name, table_name):
    """
    Retrieves primary key columns for the specified table.
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = """
        SELECT a.attname
        FROM pg_index i
        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
        WHERE i.indrelid = '{}.{}'::regclass AND i.indisprimary;
        """.format(schema_name, table_name)
        
        cursor.execute(query)
        primary_keys = [row[0] for row in cursor.fetchall()]
        
        cursor.close()
        conn.close()
        
        logger.info(f"Primary keys for {schema_name}.{table_name}: {primary_keys}")
        return primary_keys
    except Exception as e:
        logger.error(f"Error retrieving primary keys: {e}")
        raise

def read_data_from_s3(s3_path):
    """
    Reads data from an S3 location into a Spark DataFrame.
    If s3_path is None, reads from a staging table.
    """
    try:
        df = spark.read.option("header", "true").option("inferSchema", "true").csv(s3_path)
        logger.info(f"Successfully read data from {s3_path}")
        return df
    except Exception as e:
        logger.error(f"Error reading data from S3: {e}")
        raise

def read_data_from_staging(schema_name, table_name):
    """
    Reads data from a staging table in PostgreSQL.
    """
    try:
        jdbc_url, user, password = get_jdbc_url()
        df = spark.read.format("jdbc").options(
            url=jdbc_url,
            dbtable=f"{schema_name}.{table_name}",
            user=user,
            password=password
        ).load()
        logger.info(f"Successfully read data from {schema_name}.{table_name}")
        return df
    except Exception as e:
        logger.error(f"Error reading data from staging table: {e}")
        raise

def clean_dataframe(df):
    """
    Cleans the DataFrame by handling null values and empty strings.
    """
    # Replace empty strings and whitespace-only strings with nulls
    for col_name in df.columns:
        df = df.withColumn(
            col_name,
            F.when(
                (F.col(col_name).isNotNull()) & 
                (F.trim(F.col(col_name)) == ""), 
                F.lit(None)
            ).otherwise(F.col(col_name))
        )
        
        # Replace "nan" strings with nulls (case-insensitive)
        df = df.withColumn(
            col_name,
            F.when(
                (F.col(col_name).isNotNull()) & 
                (F.lower(F.trim(F.col(col_name))) == "nan"), 
                F.lit(None)
            ).otherwise(F.col(col_name))
        )
    
    logger.info("DataFrame cleaning completed")
    return df

def read_existing_records(schema_name, table_name, compare_columns, mapped_columns, df_values):
    """
    Reads existing records from PostgreSQL based on comparison columns.
    """
    try:
        # Create a temporary view of the DataFrame values for the comparison columns
        df_values.createOrReplaceTempView("compare_values")
        
        # Get JDBC connection parameters
        jdbc_url, user, password = get_jdbc_url()
        
        # Map the compare columns from DataFrame to DB columns
        db_columns = [mapped_columns.get(col, col) for col in compare_columns]
        
        # Construct SQL query conditions for the JOIN
        join_conditions = []
        for i, col in enumerate(compare_columns):
            db_col = mapped_columns.get(col, col)
            join_conditions.append(f"t.{db_col} = cv.{col}")
        
        # Construct the full SQL query
        query = f"""
        SELECT t.* 
        FROM {schema_name}.{table_name} t
        JOIN compare_values cv ON {' AND '.join(join_conditions)}
        """
        
        # Execute the query
        existing_records = spark.read.format("jdbc").options(
            url=jdbc_url,
            query=query,
            user=user,
            password=password
        ).load()
        
        return existing_records
    except Exception as e:
        logger.error(f"Error reading existing records: {e}")
        raise

def sync_dataframe_with_postgresql(df, schema_name, table_name, compare_columns, column_mapping, load_flag_column):
    """
    Syncs a PySpark DataFrame with a PostgreSQL table by performing insert or update operations
    based on the load_flag column.
    """
    try:
        # Clean the DataFrame
        df = clean_dataframe(df)
        
        # Get primary keys
        primary_keys = get_primary_keys(schema_name, table_name)
        
        # Apply column mapping
        if column_mapping is None:
            column_mapping = {}
        
        # Create reverse mapping (from DB to DataFrame columns)
        reverse_mapping = {v: k for k, v in column_mapping.items()}
        
        # Prepare the connection
        conn = get_db_connection()
        
        # Process updates (load_flag = 'U')
        update_records = df.filter(F.upper(F.col(load_flag_column)) == "U")
        
        # Process inserts (load_flag = 'I')
        insert_records = df.filter(F.upper(F.col(load_flag_column)) == "I")
        
        # Collect mismatched records
        mismatched_records = df.filter(
            (F.col(load_flag_column).isNull()) | 
            ((F.upper(F.col(load_flag_column)) != "U") & (F.upper(F.col(load_flag_column)) != "I"))
        )
        
        # Handle updates
        if update_records.count() > 0:
            logger.info("Processing update records")
            
            # Extract the comparison values
            compare_values = update_records.select(*compare_columns)
            
            # Check for existing records
            existing_records = read_existing_records(
                schema_name, table_name, compare_columns, column_mapping, compare_values
            )
            
            # Create a temporary view for the update records
            update_records.createOrReplaceTempView("update_records")
            existing_records.createOrReplaceTempView("existing_records")
            
            # Get the actual records to update by joining with existing records
            update_conditions = []
            for col in compare_columns:
                db_col = column_mapping.get(col, col)
                df_col = reverse_mapping.get(db_col, db_col)
                update_conditions.append(f"ur.{col} = er.{db_col}")
            
            records_to_update = spark.sql(f"""
                SELECT ur.* 
                FROM update_records ur
                JOIN existing_records er ON {' AND '.join(update_conditions)}
            """)
            
            # Get records that should be updated but don't exist
            records_not_found = spark.sql(f"""
                SELECT ur.* 
                FROM update_records ur
                LEFT JOIN existing_records er ON {' AND '.join(update_conditions)}
                WHERE er.{primary_keys[0]} IS NULL
            """)
            
            # Add records not found to mismatched records
            mismatched_records = mismatched_records.union(records_not_found)
            
            # Process each update record
            if records_to_update.count() > 0:
                # Convert to pandas for row-by-row processing with psycopg2
                # This is not ideal for large datasets but matches the original code's behavior
                update_pd = records_to_update.toPandas()
                
                cursor = conn.cursor()
                
                for _, row in update_pd.iterrows():
                    # Prepare update data
                    update_data = {}
                    where_clause = []
                    
                    # Build WHERE clause
                    for col in compare_columns:
                        db_col = column_mapping.get(col, col)
                        where_clause.append(f"{db_col} = %s")
                        
                    # Build SET clause
                    set_values = []
                    params = []
                    
                    for col in row.index:
                        if col not in compare_columns and column_mapping.get(col, col) not in primary_keys and col != load_flag_column and col != "created_by":
                            db_col = column_mapping.get(col, col)
                            set_values.append(f"{db_col} = %s")
                            params.append(row[col])
                    
                    # Add audit fields
                    set_values.append("last_updated_by = %s")
                    params.append(row["created_by"])
                    
                    set_values.append("last_updated_date = %s")
                    params.append(date.today())
                    
                    # Add WHERE clause parameters
                    for col in compare_columns:
                        params.append(row[col])
                    
                    # Execute update query
                    update_query = f"""
                    UPDATE {schema_name}.{table_name}
                    SET {', '.join(set_values)}
                    WHERE {' AND '.join(where_clause)}
                    """
                    
                    cursor.execute(update_query, params)
                
                cursor.close()
                conn.commit()
                logger.info(f"Updated {records_to_update.count()} records")
        
        # Handle inserts
        if insert_records.count() > 0:
            logger.info("Processing insert records")
            
            # Extract the comparison values
            compare_values = insert_records.select(*compare_columns)
            
            # Check for existing records
            existing_records = read_existing_records(
                schema_name, table_name, compare_columns, column_mapping, compare_values
            )
            
            # Create a temporary view for the insert records
            insert_records.createOrReplaceTempView("insert_records")
            existing_records.createOrReplaceTempView("existing_records")
            
            # Get the actual records to insert by excluding existing records
            insert_conditions = []
            for col in compare_columns:
                db_col = column_mapping.get(col, col)
                df_col = reverse_mapping.get(db_col, db_col)
                insert_conditions.append(f"ir.{col} = er.{db_col}")
            
            records_to_insert = spark.sql(f"""
                SELECT ir.* 
                FROM insert_records ir
                LEFT JOIN existing_records er ON {' AND '.join(insert_conditions)}
                WHERE er.{primary_keys[0]} IS NULL
            """)
            
            # Get records that already exist but marked for insert
            records_already_exist = spark.sql(f"""
                SELECT ir.* 
                FROM insert_records ir
                JOIN existing_records er ON {' AND '.join(insert_conditions)}
            """)
            
            # Add records that already exist to mismatched records
            mismatched_records = mismatched_records.union(records_already_exist)
            
            # Process each insert record
            if records_to_insert.count() > 0:
                # Convert to pandas for row-by-row processing with psycopg2
                insert_pd = records_to_insert.toPandas()
                
                cursor = conn.cursor()
                
                for _, row in insert_pd.iterrows():
                    # Prepare insert data
                    column_names = []
                    placeholders = []
                    params = []
                    
                    for col in row.index:
                        if col != load_flag_column:
                            db_col = column_mapping.get(col, col)
                            column_names.append(db_col)
                            placeholders.append("%s")
                            params.append(row[col])
                    
                    # Add audit fields if not already present
                    if "is_active" not in column_names:
                        column_names.append("is_active")
                        placeholders.append("%s")
                        params.append(True)
                    
                    if "last_updated_by" not in column_names:
                        column_names.append("last_updated_by")
                        placeholders.append("%s")
                        params.append(row["created_by"])
                    
                    if "last_updated_date" not in column_names:
                        column_names.append("last_updated_date")
                        placeholders.append("%s")
                        params.append(date.today())
                    
                    # Execute insert query
                    insert_query = f"""
                    INSERT INTO {schema_name}.{table_name} ({', '.join(column_names)})
                    VALUES ({', '.join(placeholders)})
                    """
                    
                    cursor.execute(insert_query, params)
                
                cursor.close()
                conn.commit()
                logger.info(f"Inserted {records_to_insert.count()} records")
        
        # Handle mismatched records
        if mismatched_records.count() > 0:
            logger.info(f"Found {mismatched_records.count()} mismatched records")
            mismatched_path = f"s3://{ss.s3_bucket}/mismatch_records/"
            mismatched_records.write.mode("overwrite").csv(mismatched_path, header=True)
            logger.info(f"Mismatched records written to {mismatched_path}")
        
        # Close the connection
        conn.close()
        
        return {
            "total_records": df.count(),
            "updated_records": update_records.count() if 'update_records' in locals() else 0,
            "inserted_records": insert_records.count() if 'insert_records' in locals() else 0,
            "mismatched_records": mismatched_records.count()
        }
    except Exception as e:
        logger.error(f"Error in sync_dataframe_with_postgresql: {e}")
        raise

def main():
    """
    Main function to orchestrate the data synchronization process.
    """
    try:
        # Get job name from arguments
        job_args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        # Define parameters directly in the code
        schema_name = "sds_main"
        table_name = "business_entity_card_revenues"
        compare_columns = ["stg_business_entity_id", "end_date"]
        column_mapping = {  # Map only the mismatched column
            "stg_business_entity_id": "business_entity_id"
        }
        load_flag_column = "load_flag"
        
        # Option 1: Read from S3
        #s3_path = f"s3://{ss.s3_bucket}/card_revenues/card_revenues.csv"
        s3_path = "s3://app-id-111597-dep-id-114116-uu-id-by081rbjj1vo/upsert/card_revenues.csv"
        df = read_data_from_s3(s3_path)
        
        # Option 2: Read from staging table
        #staging_schema = "sds_staging"
        #staging_table = "business_entity_card_revenues"
        #df = read_data_from_staging(staging_schema, staging_table)
        
        logger.info(f"Data read successfully with {df.count()} rows")
        
        # Call the sync function
        result = sync_dataframe_with_postgresql(
            df=df,
            schema_name=schema_name,
            table_name=table_name,
            compare_columns=compare_columns,
            column_mapping=column_mapping,
            load_flag_column=load_flag_column
        )
        
        logger.info(f"Sync operation completed: {result}")
        
    except Exception as e:
        logger.error(f"Error in main function: {e}")
        raise

if __name__ == "__main__":
    main()
